{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRvCuHz36wh9"
      },
      "source": [
        "# Analiza danych tekstowych -- wstÄ™p\n",
        "\n",
        "## Åšrodowisko analityczne\n",
        "\n",
        "Przed Wami krÃ³tkie wprowadzenie do podstwowych zadaÅ„ z obszaru NLP. Wykonamy je na dostÄ™pnych danych, maÅ‚o wymagajÄ…cym pipelinie i dla jÄ™zyka angielskiego. Ale uwaga: analogiczne zadania bÄ™dziecie robiÄ‡ dla jÄ™zyka polskiego (praca samodzielna, analogiczna do prezentowanej dla jÄ™zyka angielskiego).\n",
        "\n",
        "BÄ™dziemy uÅ¼ywaÄ‡ pakietu `spacy`, ktÃ³ry sprawdza siÄ™ przy analizie tekstu, `scikit-learn` do obliczeÅ„ i `matplotlip`, ktÃ³ry pomoÅ¼e zaprezentowaÄ‡ dane na wykresach. Jako dane anglojÄ™zyczne weÅºmiemy sobie `en_core_web_sm` --> model od SpaCy trenowany na tekstach newsÃ³w z jÄ™zyka angielskiego. Pierwsze zadanie dla Was: Jaki pakiet danych odpowiada za dane z jÄ™zyka polskiego?\n",
        "\n",
        "ModuÅ‚ `datasets` odpowiada za Å‚atwe Å‚adowanie danych. Dane pochodzÄ… z [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6vQYVbzRT2b"
      },
      "outputs": [],
      "source": [
        "!pip install spacy scikit-learn matplotlib datasets\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "b-4JZPM9X5xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sPaOFZMRSku"
      },
      "source": [
        "## Tokenizacja\n",
        "\n",
        "Pierwsze zadanie polega na stokenizowaniu tekstu, co oznacza ni mniej ni wiÄ™cej jak podziaÅ‚ tekstu na najmniejsze znaczÄ…ce elementy, ktÃ³re wspÃ³lnie tworzÄ… wyrazy tekstowe i tekst. Tokenizacja dla kaÅ¼dego jÄ™zyka jest inna, co moÅ¼ecie zaobserwowaÄ‡ zestawiajÄ…c dane dla jÄ™zyka angielskiego z danymi dla jÄ™zyka polskiego i porÃ³wnujÄ…c wyniki uzyskane na tekstach.\n",
        "\n",
        "Tokenizacja przydaje siÄ™ w Å¼yciu. PracujÄ…c na tokenach, pracujemy na uniwersalnych danych i porÃ³wnujemy dane w sposÃ³b niestronniczy.\n",
        "\n",
        "PorÃ³wnajcie zdania:\n",
        "\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees. It's all we need to have some fun in life.\"\n",
        "\"ChciaÅ‚abym, Å¼ebyÅ›cie mieli tyle zabawy z Ä‡wiczeniami, ile ubawu majÄ… myszy z kaÅ¼dym jednym kawaÅ‚eczkiem sera. Wszystko, czego potrzebujemy w Å¼yÄ‡ku, to zabawa.\"\n",
        "\n",
        "\n",
        "WyjaÅ›nienie:\n",
        "Dowiadujemy siÄ™ tu jak tokenizowaÄ‡ tekst, a za chwilÄ™ dowiemy siÄ™, Å¼e te tokeny mogÄ… byÄ‡ przetwarzane w wektory, na podstawie ktÃ³rych moÅ¼emy przewidywaÄ‡ kolejne wyrazy tekstowe. JeÅ›li jednak wynik tokenizacji nie ma byÄ‡ uÅ¼yty bezpoÅ›rednio jako dane wejÅ›ciowe sieci neuronowej, moÅ¼emy uÅ¼yÄ‡ przyjaznej tokenizacji z pakietu `spacy`.\n",
        "\n",
        "Zainicjujmy wiÄ™c pakiet `spacy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZr9Oy8P7SHE"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "R3ELyRc2zaJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfcttssX8Q16"
      },
      "source": [
        "Najpierw stokenizujmy tekst dla jÄ™zyka angielskiego:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4a1_8Tf8Vwc"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees. It's all we need to have some fun in life.\"\"\"\n",
        "\n",
        "tokens_en = nlp_en(text)\n",
        "[token.text for token in tokens_en]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A potem dla polskiego:\n",
        "(pamiÄ™tajmy o innej nazwie zmiennych!)"
      ],
      "metadata": {
        "id": "PRB7_tIjzujG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "4S6_NcsdzqvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y06DuFMVDmWO"
      },
      "source": [
        "A teraz podzielmy teksty na zdania."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq9wbNqGDoox",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "[sentence.text for sentence in tokens_en.sents]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "mFve78CiY9NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### â­ Zadanie sprawdzajÄ…ce umiejÄ™tnoÅ›ciâ­\n",
        "\n",
        "StwÃ³rz zdanie, ktÃ³re:\n",
        "- zawiera skrÃ³ty (USA, U.S., i.e., ww.)\n",
        "- zawiera nazwy (McDonald's, Kelly's)\n",
        "- zawiera czasowniki w formach warunkowych, przypuszczajÄ…cych (I would like, chciaÅ‚abym).\n",
        "\n",
        "```\n",
        "np. We have been to U.K. before we got to the very special country, i.e. Poland.\n",
        "```"
      ],
      "metadata": {
        "id": "mNlfmjT60NLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdKGB9UB-N3Z"
      },
      "outputs": [],
      "source": [
        "# Miejsce na TwÃ³j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z39blSV0-sof"
      },
      "source": [
        "## Wykrywanie kategorii morfologicznych\n",
        "\n",
        "`spacy` moÅ¼na uÅ¼ywaÄ‡ teÅ¼ do analizy kategorii morfologicznych (morfoskÅ‚adniowych, zwanych teÅ¼ *czÄ™Å›ciami mowy*, ktÃ³re -- o zgrozo -- odmieniajÄ… siÄ™ przez przypadki, osoby i stopnie). Tagowanie morfoskÅ‚adniowe, inaczej Part-of-Speech Tagging (POS Tagging) przydaje siÄ™ w bardziej zaawansowanych zadaniach, moÅ¼e teÅ¼ pozwoliÄ‡ na wnikliwy wglÄ…d w wÅ‚aÅ›ciwoÅ›ci tekstu.\n",
        "\n",
        "W tym zadaniu moÅ¼emy uÅ¼yÄ‡ tokenÃ³w (`tokens_en`) z poprzednich Ä‡wiczeÅ„."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqCgDjRt-zMG"
      },
      "outputs": [],
      "source": [
        "[(token.text, token.pos_) for token in tokens_en]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "3_s6GPdoZJPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0J9krYr_qal"
      },
      "source": [
        "### â­ Zadanie sprawdzajÄ…ce umiejÄ™tnoÅ›ciâ­\n",
        "\n",
        "**A teraz** zobaczmy, ile i jakich tagÃ³w (w tekstach moÅ¼na znaleÅºÄ‡ teÅ¼ okreÅ›lenie POS tagÃ³w) mamy w naszych przykÅ‚adach. Jakie problemy mogÄ… byÄ‡ poruszone przy okazji takiego zadania? Jak zrobiÄ‡ wykres? (Za wykres jest bonus ğŸ“Š ğŸ˜€)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihx1WDro_wKH"
      },
      "outputs": [],
      "source": [
        "# Miejsce na TwÃ³j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAgdEyZI_02h"
      },
      "source": [
        "Lematyzacja\n",
        "Wchodzimy na kolejny poziom abstrakcji. Liczymy teraz nie tokeny, a typy. ZaÅ‚Ã³Å¼my, Å¼e chcemy policzyÄ‡ dzieci z przedszkola bawiÄ…ce siÄ™ gÅ‚oÅ›no na placu zabaw za naszym oknem. Chcemy dowiedzieÄ‡ siÄ™, kto krzyczy gÅ‚oÅ›niej, dziewczynki czy chÅ‚opcy. W tym celu przyglÄ…damy siÄ™ kaÅ¼demu dziecku i stwierdzamy, czy jest chÅ‚opcem czy dziewczynkÄ…. Sprowadzamy patrzenie na obiekt do binarnego wyboru pÅ‚ci, czasem mamy wÄ…tpliwoÅ›ci, co jest normalne. MoÅ¼emy (z wahaniem lub bez) powiedzieÄ‡, Å¼e na placu zabaw sÄ… dwa typy dzieci: krzyczÄ…ce i nie, dziewczynki i chÅ‚opcy. I podobnie jest z lematyzacjÄ…, choÄ‡ trochÄ™ inaczej. Å»eby zobaczyÄ‡, co jest w zdaniu, musimy przyjrzeÄ‡ siÄ™ okazom i stwiedziÄ‡, Å¼e jeÅ›li w tekÅ›cie dziecko drze siÄ™ (jakby jutra nie byÅ‚o), darÅ‚o siÄ™ (aÅ¼ do momentu, kiedy nie zagÅ‚uszyÅ‚y go syreny policyjne) lub bÄ™dzie siÄ™ darÅ‚o (do ukoÅ„czenia 18 roku Å¼ycia), to mÃ³wimy o jednej czynnoÅ›ci darcia siÄ™ wyraÅ¼onej jako rÃ³Å¼ne formy czasownika DRZEÄ† SIÄ˜. To jak w sÅ‚owniku. JeÅ›li chcemy sprawdziÄ‡ pisowniÄ™, szukamy czegoÅ› co jest w mianowniku liczby pojedynczej i rodzaju mÄ™skim, albo w bezokoliczniku, albo w stopniu rÃ³wnym i teÅ¼ rodzaju mÄ™skim.\n",
        "\n",
        "Co i kiedy liczymy? WypisujÄ…c wszystkie elementy (tokeny), liczyliÅ›my budulec tekstu. WypisujÄ…c lematy (typy), liczymy uÅ¼ycie konkretnych pojÄ™Ä‡ niezaleÅ¼nie od ich formy.\n",
        "\n",
        "JeÅ›li chcesz policzyÄ‡, ile sÅ‚Ã³w zostaÅ‚o wymienionych w tekÅ›cie, bardzo przydatne jest sprowadzenie wszystkich sÅ‚Ã³w do ich form podstawowych. Proces ten nazywany jest lematyzacjÄ…. Tekst przetworzony za pomocÄ… spacy zawiera juÅ¼ lematy dla kaÅ¼dego tokena. Wykorzystamy tÄ™ technikÄ™ w dalszej czÄ™Å›ci laboratorium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIEpFin0_3cE"
      },
      "outputs": [],
      "source": [
        "[(token.text, token.lemma_) for token in tokens_en]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "AXk28TGz0r1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Y0w9TwHxiq"
      },
      "source": [
        "â­ Zadanie sprawdzajÄ…ce umiejÄ™tnoÅ›ci â­\n",
        "> Dla nietypowych rzeczownikÃ³w w jÄ™zyku angielskim:\n",
        "\n",
        "* entities\n",
        "* was\n",
        "* mice\n",
        "*cacti\n",
        "* octopi\n",
        "\n",
        "znajdÅº lematy i oceÅ„, czy spacy rozpoznaÅ‚ je poprawnie. Czy moÅ¼esz wskazaÄ‡ analogiczne przykÅ‚ady dla polskiego?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPMnY3xpH6ZT"
      },
      "outputs": [],
      "source": [
        "# Miejsce na TwÃ³j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mih5AZl5_35c"
      },
      "source": [
        "## Named entity recognition\n",
        "\n",
        "Analiza tekstu przy uÅ¼yciu `spacy` moÅ¼e byÄ‡ bardziej zaawansowana (do tej pory analizowaliÅ›my skÅ‚adniÄ™, teraz czas na odrobinÄ™ semantyki). Takim bardziej zÅ‚oÅ¼onym zadaniem jest rozpoznawanie encji nazwanych (NER), a wiÄ™c pewnego typu obiektÃ³w, ktÃ³re mogÄ… byÄ‡ nazwÄ… wÅ‚asnÄ…, ale wcale nie muszÄ….\n",
        "\n",
        "### Zatem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqAvaZj_Cgiv"
      },
      "outputs": [],
      "source": [
        "ner_result = nlp(\"\"\"Israel is being urged by the international community - including close ally the US - to do more to limit civilian casualties.\n",
        "\n",
        "Hamas officials say at least 16,248 people have been killed in Gaza since the start of the conflict, about three quarters of them women and children.\n",
        "\n",
        "Defending Israel's war strategy, former PM Naftali Bennett has told the BBC that Israel has been showing restraint in Gaza.\n",
        "\n",
        "If we wanted to harm civilians, we could have won the whole war in one day on October 8th, he said.\n",
        "\n",
        "We could have indiscriminately bombed Gaza.\n",
        "\n",
        "It could have been the easiest thing in the world... [but] we're not doing that.\"\"\")\n",
        "[(e.text, e.label_, e.start_char, e.end_char) for e in ner_result.ents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONbTGgU-Ex_0"
      },
      "source": [
        "KaÅ¼da kategoria ma w SpaCy rozwiniÄ™cie:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGruXB_UEgHn"
      },
      "outputs": [],
      "source": [
        "spacy.explain('GPE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Woivn9naE9Al"
      },
      "source": [
        "â­ Zadanie sprawdzajÄ…ce umiejÄ™tnoÅ›ci â­\n",
        "\n",
        "> Zobacz na to samo zadanie, ale w jÄ™zyku polskim.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlkMZSw6FDPF"
      },
      "outputs": [],
      "source": [
        "# Miejsce na TwÃ³j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â­ Zadanie sprawdzajÄ…ce umiejÄ™tnoÅ›ci â­\n",
        "\n",
        "ZnajdÅº tekst, ktÃ³ry zawiera typ `WORK_OF_ART`."
      ],
      "metadata": {
        "id": "9RO0Z_uJ1hNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "UgPECLjt1j3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RM3w_wFJsc"
      },
      "source": [
        "### Obrazki\n",
        "ModuÅ‚ displacy odpowiada za wizualizacjÄ™ wynikÃ³w dziaÅ‚ania NERa. PodkreÅ›lenie / wyrÃ³Å¼nienie kolorystyczne sprawiajÄ…, Å¼e Å‚atwiej odczytaÄ‡ wyniki, Å¼eby pobieÅ¼nie je przeanalizowaÄ‡."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHBUbGOuFt2y"
      },
      "outputs": [],
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xNIdIyLGY0I"
      },
      "source": [
        "KorzystajÄ…c z moduÅ‚u `displacy` moÅ¼emy teÅ¼ przeszukiwaÄ‡ informacje pod kÄ…tem jednej wybranej lub kilku poÅ¼Ä…danych kateogrii. Å»eby tego dokonaÄ‡, musimy skorzystaÄ‡ z funkcji `displacy.render`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr8o6bPUGi5l"
      },
      "outputs": [],
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True, options={\"ents\": [\"PERSON\", \"DATE\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7UQMXj7GAaK"
      },
      "source": [
        "#### â­ Zadanie sprawdzajÄ…ce umiejÄ™tnoÅ›ci â­\n",
        "\n",
        "SprÃ³buj przeanalizowaÄ‡ dÅ‚uÅ¼szy tekst za pomocÄ… `spacy` i zwizualizuj wynik NER za pomocÄ… `displacy`. UÅ¼yj jakiegoÅ› artykuÅ‚u znalezionego w sieci.\n",
        "\n",
        "NastÄ™pnie policz ile razy kaÅ¼dy typ encji zostaÅ‚ wykryty w tekÅ›cie i wyÅ›wietl statystyki. Dodatkowy bonus za wykres ğŸ“Š ğŸ˜€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaciXADYZ6Qp"
      },
      "outputs": [],
      "source": [
        "# Miejsce na TwÃ³j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPYmmqLqG5YI"
      },
      "source": [
        "## Wykrywanie podobieÅ„stwa tekstÃ³w\n",
        "\n",
        "### Bag of words\n",
        "\n",
        "Do tej pory analizowaliÅ›my problem interpretacji formy tekstu, wskazywania konkretnych struktur i przypisywania ich do specyficznej klasy. Teraz zajmiemy siÄ™ problemem interpretacji znaczenia. Przyjrzyjmy siÄ™ rÃ³Å¼nym tekstom i sprawdÅºmy, jak te teksty sÄ… do siebie podobne. Dla uproszczenia tekstem bÄ™dÄ… pojedyncze zdania.\n",
        "\n",
        "> The quick brown fox jumps over the lazy dog.\n",
        "\n",
        "> The dog kept barking over the night.\n",
        "\n",
        "> A lazy fisherman with his dog met a fox last night.\n",
        "\n",
        "Przy nieduÅ¼ej liczbie tekstÃ³w jesteÅ›my w stanie naocznie porÃ³wnaÄ‡ prÃ³bki i okreÅ›liÄ‡ ich podobieÅ„stwo. Czym jest owo podobieÅ„stwo? GdybyÅ›my mieli scharakteryzowaÄ‡ sposoby, na jakie teksty sÄ… podobne, jakbyÅ›my je okreÅ›lili?\n",
        "\n",
        "Bardzo czÄ™sto stosowanym sposobem na znalezienie zdefiniowanego podobieÅ„stwa jest technika zwana *bag of words* (https://en.wikipedia.org/wiki/Bag-of-words_model). Polega ona na obliczeniu czÄ™stotliwoÅ›ci wystÄ™powania sÅ‚Ã³w we wszystkich tekstach, uporzÄ…dkowaniu tekstu wg najpopularniejszych z nich, a nastÄ™pnie przedstawieniu tekstu jako listy liczb caÅ‚kowitych zawierajÄ…cych liczbÄ™ wystÄ…pieÅ„ tych sÅ‚Ã³w. Co waÅ¼ne, to podejÅ›cie zupeÅ‚nie ignoruje\n",
        "\n",
        "PrzykÅ‚ad lepszy niÅ¼ wykÅ‚ad!\n",
        "\n",
        "Do obliczenia metryk tekstowych uÅ¼yjemy moduÅ‚u `sklearn`. Klasa `CountVectorizer` wykonuje wszystkie obliczenia za nas. Parametr `max_features=5` mÃ³wi wektoryzatorowi, Å¼e chcemy wybraÄ‡ co najwyÅ¼ej 5 najpopularniejszych tokenÃ³w ze wszystkich tekstÃ³w.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myGn2xGKI9hr"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog kept barking very loud barking and barking again over the night.\",\n",
        "    \"A lazy fisherman with his dog met a fox last night.\",\n",
        "]\n",
        "\n",
        "count_vector = CountVectorizer(max_features=5)\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "data_count.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "e6Y9j6R9cVNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMeNodp3Kj8y"
      },
      "source": [
        "OK, co oznaczajÄ… dane liczbowe, ktÃ³re uzyskaliÅ›my?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0ABs1ziKyDA"
      },
      "outputs": [],
      "source": [
        "count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na TwÃ³j kod"
      ],
      "metadata": {
        "id": "1QLo1YXWc2cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fme7G6lDK1Eg"
      },
      "source": [
        "# Tokeny dla angielskiego to:\n",
        "\n",
        "```\n",
        "[ 'barking', 'dog', 'fox', 'over', 'the']\n",
        "```\n",
        "\n",
        "z reprezentacjÄ… wektorowÄ…:\n",
        "\n",
        "```\n",
        "array([[1, 1, 1, 0, 2],\n",
        "       [1, 0, 0, 1, 2],\n",
        "       [1, 1, 1, 1, 0]])\n",
        "```\n",
        "\n",
        "Co oznacza, Å¼e:\n",
        "* sÅ‚owo `barking` pojawia siÄ™ w ogÃ³le trzy razy, ale w jednym zdaniu\n",
        "* sÅ‚owo `dog` w pierwszym tekÅ›cie pojawia siÄ™ tylko raz, podobnie w trzecim\n",
        "* sÅ‚owo `fox` pojawia siÄ™ raz w pierwszym i raz w trzecim tekÅ›cie\n",
        "* sÅ‚owo `over` nie wystÄ™puje w pierwszym tekÅ›cie\n",
        "* sÅ‚owo `the` pojawia siÄ™ dwukrotnie w pierwszym i drugim tekÅ›cie, w trzecim nie wystÄ™puje wcale\n",
        "\n",
        "Now you should understand the *bag of words* text representation. We can say that the more similar the vectors are, the more similar the texts are, too. We can obviously calculate the distance between them and even visualize them on a chart, but we need a few more exercies and obviously - more data!\n",
        "\n",
        "#### â­ Zadania sprawdzajÄ…ce umiejÄ™tnoÅ›ci â­\n",
        "\n",
        "PrzeprowadÅº analogiczny eksperyment dla jÄ™zyka polskiego i wyciÄ…gnij wnioski podobnie jak w powyÅ¼szym Ä‡wiczeniu.\n",
        "\n",
        "ZwrÃ³Ä‡ uwagÄ™ na parametr `max_features` i sprÃ³buj zmieniaÄ‡ jego wartoÅ›Ä‡, obserwujÄ…c zmianÄ™ reprezentacji wynikÃ³w. WyciÄ…gnij wnioski na temat przeÅ‚oÅ¼enia wartoÅ›ci parametru na jakoÅ›Ä‡ prezentowanych wynikÃ³w."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe2_07hOMKbO"
      },
      "outputs": [],
      "source": [
        "# Miejsce na TwÃ³j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modele typu transformers korzystajÄ… z modyfikacji tego podejÅ›cia. WiÄ™cej o rozwiÄ…zaniu problemu podobieÅ„stwa tu: https://huggingface.co/tasks/sentence-similarity."
      ],
      "metadata": {
        "id": "UEB6xm4jPzaR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0EMySonML50"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Semantyczne rozwaÅ¼ania nad jÄ™zykiem prowadzÄ… do obserwacji, Å¼e sÅ‚owa nie przekazujÄ… porÃ³wnywalnie istotnych informacji. Co to znaczy? W przypadku przykÅ‚adu z jÄ™zyka angielskiego, sÅ‚owo `the` mÃ³wi nam nieco mniej niÅ¼ sÅ‚owo `dog` czy `lazy`, a jednak to ono pojawia siÄ™ w tekstach najczÄ™Å›ciej. Nie oznacza to oczywiÅ›cie, Å¼e to sÅ‚owo nic nie znaczy bÄ…dÅº nie peÅ‚ni w systemie jÄ™zykowym istotnej funkcji. W tym zadaniu rozpatrujemy tylko istotnoÅ›Ä‡ sÅ‚owa w przeÅ‚oÅ¼eniu na znaczenie caÅ‚ego tekstu lub grupy tekstÃ³w i przez taki pryzmat bÄ™dziemy patrzeÄ‡ na `stopwords`, a wiÄ™c sÅ‚owa popularne, budulce tekstu, nienosÄ…ce znaczenia.\n",
        "`Stopwords` obsÅ‚ugiwane sÄ… w pakiecie `SpaCy` przez moduÅ‚ `sklearn` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), ktÃ³ry identyfikuje je, a nastÄ™pnie usuwa z reprezentacji zbioru danych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkhFQzFdMs7U"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "list(ENGLISH_STOP_WORDS)[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHOezUD9OHnJ"
      },
      "source": [
        "Nie musisz importowaÄ‡ stopwords, aby z nich korzystaÄ‡, poniewaÅ¼ sÄ… one zarzÄ…dzane wewnÄ™trznie w pakiecie (`_` w nazwie pakietu).\n",
        "\n",
        "Teraz wszystko, co musisz zrobiÄ‡, to zdefiniowaÄ‡ wbudowanÄ… listÄ™ stopwords, ktÃ³rych chcesz uÅ¼yÄ‡ przed obliczeniem wektorÃ³w."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSzlqDvFOSbH"
      },
      "outputs": [],
      "source": [
        "count_vector = CountVectorizer(max_features=10, stop_words = 'english')\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L52dVF9qO_N3"
      },
      "source": [
        "### Wizualizacja danych\n",
        "\n",
        "\n",
        "Wykrywanie podobnych tekstÃ³w w przypadku duÅ¼ej iloÅ›ci danych moÅ¼e stanowiÄ‡ wyzwanie. Zawsze pomocna jest wizualizacja danych na ekranie, wiÄ™c moÅ¼emy wykreÅ›liÄ‡ wektory i sprawdziÄ‡, czy moÅ¼emy wykryÄ‡ jakieÅ› grupy na ekranie. BÄ™dzie to trudne w przypadku trzech tekstÃ³w, na ktÃ³rych obecnie pracujemy, ale zrozumiesz ideÄ™.\n",
        "\n",
        "MoÅ¼emy teraz zawiesiÄ‡ to laboratorium i poczekaÄ‡ do 2048 roku, kiedy ekrany 5D bÄ™dÄ… dostÄ™pne, lub uÅ¼yÄ‡ popularnego algorytmu `t-SNE` do *spÅ‚aszczenia* danych, a nastÄ™pnie ich wizualizacji. Wybierzemy drugie rozwiÄ…zanie ğŸ˜‰.\n",
        "\n",
        "Nie zagÅ‚Ä™biajÄ…c siÄ™ zbytnio w dziaÅ‚anie tego algorytmu, jest on w stanie zredukowaÄ‡ wektory XD do wektorÃ³w YD, z X>Y, zachowujÄ…c odlegÅ‚oÅ›ci miÄ™dzy nimi. W przypadku naszego tekstu chcemy zredukowaÄ‡ wektory 5D (5 cech tekstu) do wektorÃ³w 2D (czyli do formatu, ktÃ³ry moÅ¼na wykreÅ›liÄ‡ na ekranie).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6c2W1S_QGIr"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne_model = TSNE(n_components=2, perplexity=2)\n",
        "tsne_data = tsne_model.fit_transform(data_count.toarray())\n",
        "\n",
        "tsne_data\n",
        "#data_count.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn5GI1iFQtr1"
      },
      "source": [
        "Algortym zredukowaÅ‚ wektory, co moÅ¼emy zobaczyÄ‡, zamiast musieÄ‡ sobie to wyobraÅ¼aÄ‡."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbwqxktaQ0gd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
        "\n",
        "for i, label in enumerate([\"quick fox\", \"barking dog\", \"lazy fisherman\"]):\n",
        "    ax.annotate(label, (tsne_data[i, 0], tsne_data[i, 1]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I60UgASRT70i"
      },
      "source": [
        "IstniejÄ… tylko trzy punkty danych, wiÄ™c trudno powiedzieÄ‡, czy teksty moÅ¼na uznaÄ‡ za podobne do siebie, czy nie. GdybyÅ›my jednak mieli znacznie wiÄ™cej tekstÃ³w, moglibyÅ›my podejrzewaÄ‡, Å¼e punkty danych utworzyÅ‚yby pewne rozrÃ³Å¼nialne grupy, co oznacza, Å¼e teksty mÃ³wiÄ… o podobnych tematach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvhm1-manrMN"
      },
      "source": [
        "## Zbiory danych\n",
        "\n",
        "Do ostatniego zadania potrzebujemy wiÄ™cej danych, Å¼eby zaobserwowaÄ‡ proces klasteryzacji. JednÄ… z moÅ¼liwych drÃ³g pozyskania danych jest skorzystanie z moduÅ‚u [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html) `datasets`, aby pobraÄ‡ kilka tekstÃ³w, nad ktÃ³rymi moÅ¼emy pracowaÄ‡.\n",
        "\n",
        "Zobaczmy, co jest w Å›rodku."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU9xQv2RoAs-"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "datasets.list_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wyhw7igoQdD"
      },
      "source": [
        "ZbiorÃ³w danych jest sporo, a ich liczba stale roÅ›nie. Na potrzeby tego eksperymentu wybierzmy dowolny zbiÃ³r (zadanie nie jest tak wyspecyfikowane, by nakÅ‚adaÄ‡ na nas koniecznoÅ›Ä‡ wyboru wg konkretnych kryteriÃ³w).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GgJ05UnoclR"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset('ag_news', split='train')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU8czV0vqBkG"
      },
      "source": [
        "Jak widzieliÅ›my w poprzednich przykÅ‚adach, lista tekstÃ³w bÄ™dzie na razie Å‚atwiejszÄ… strukturÄ… do pracy. MajÄ…c powyÅ¼szy zbiÃ³r danych z polami `text` i `label`, moÅ¼emy utworzyÄ‡ listÄ™ tekstÃ³w z prostym zrozumieniem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GyDWshMqSHq"
      },
      "outputs": [],
      "source": [
        "large_texts = [item['text'] for item in dataset]\n",
        "large_texts[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYYikmVXUM9Y"
      },
      "source": [
        "## â­ Zadanie sprawdzajÄ…ce umiejÄ™tnoÅ›ci ğŸ—» â­\n",
        "\n",
        "Masz wszystkie narzÄ™dzia!\n",
        "\n",
        "Zbierz duÅ¼y zbiÃ³r danych tekstÃ³w z *HF* i:\n",
        "\n",
        "1.   Przygotuj je do analizy, np.\n",
        "  \n",
        "  a. Stokenizuj je.\n",
        "\n",
        "  b. PrzeksztaÅ‚Ä‡ tokeny w lematy (tak, aby `dog` i `dogs` byÅ‚y traktowane jako ta sama funkcja).\n",
        "2. Przedstaw teksty jako bag of words, pamiÄ™tajÄ…c o stopwords. Eksperymentuj z liczbÄ… cech. JeÅ›li okaÅ¼e siÄ™, Å¼e istniejÄ… cechy, ktÃ³re wpÅ‚ywajÄ… na reprezentacjÄ™, wrÃ³Ä‡ do kroku 1. i weÅº to pod uwagÄ™ podczas przygotowywania danych.\n",
        "3. Zwizualizuj dane na wykresie (bez etykiet dla lepszej wydajnoÅ›ci). Czy moÅ¼esz wyrÃ³Å¼niÄ‡ jakieÅ› grupy tekstÃ³w? O czym sÄ… te teksty?\n",
        "4. Wykryj nazwane jednostki w reprezentantach grup. Czy nazwane jednostki sugerujÄ… rÃ³wnieÅ¼ temat tekstu?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkIYNxKC4D0c"
      },
      "outputs": [],
      "source": [
        "# Miejsce na TwÃ³j kod"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}