{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRvCuHz36wh9"
      },
      "source": [
        "# Analiza danych tekstowych -- wstęp\n",
        "\n",
        "## Środowisko analityczne\n",
        "\n",
        "Przed Wami krótkie wprowadzenie do podstwowych zadań z obszaru NLP. Wykonamy je na dostępnych danych, mało wymagającym pipelinie i dla języka angielskiego. Ale uwaga: analogiczne zadania będziecie robić dla języka polskiego (praca samodzielna, analogiczna do prezentowanej dla języka angielskiego).\n",
        "\n",
        "Będziemy używać pakietu `spacy`, który sprawdza się przy analizie tekstu, `scikit-learn` do obliczeń i `matplotlip`, który pomoże zaprezentować dane na wykresach. Jako dane anglojęzyczne weźmiemy sobie `en_core_web_sm` --> model od SpaCy trenowany na tekstach newsów z języka angielskiego. Pierwsze zadanie dla Was: Jaki pakiet danych odpowiada za dane z języka polskiego?\n",
        "\n",
        "Moduł `datasets` odpowiada za łatwe ładowanie danych. Dane pochodzą z [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6vQYVbzRT2b"
      },
      "outputs": [],
      "source": [
        "!pip install spacy scikit-learn matplotlib datasets\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "b-4JZPM9X5xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sPaOFZMRSku"
      },
      "source": [
        "## Tokenizacja\n",
        "\n",
        "Pierwsze zadanie polega na stokenizowaniu tekstu, co oznacza ni mniej ni więcej jak podział tekstu na najmniejsze znaczące elementy, które wspólnie tworzą wyrazy tekstowe i tekst. Tokenizacja dla każdego języka jest inna, co możecie zaobserwować zestawiając dane dla języka angielskiego z danymi dla języka polskiego i porównując wyniki uzyskane na tekstach.\n",
        "\n",
        "Tokenizacja przydaje się w życiu. Pracując na tokenach, pracujemy na uniwersalnych danych i porównujemy dane w sposób niestronniczy.\n",
        "\n",
        "Porównajcie zdania:\n",
        "\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees. It's all we need to have some fun in life.\"\n",
        "\"Chciałabym, żebyście mieli tyle zabawy z ćwiczeniami, ile ubawu mają myszy z każdym jednym kawałeczkiem sera. Wszystko, czego potrzebujemy w żyćku, to zabawa.\"\n",
        "\n",
        "\n",
        "Wyjaśnienie:\n",
        "Dowiadujemy się tu jak tokenizować tekst, a za chwilę dowiemy się, że te tokeny mogą być przetwarzane w wektory, na podstawie których możemy przewidywać kolejne wyrazy tekstowe. Jeśli jednak wynik tokenizacji nie ma być użyty bezpośrednio jako dane wejściowe sieci neuronowej, możemy użyć przyjaznej tokenizacji z pakietu `spacy`.\n",
        "\n",
        "Zainicjujmy więc pakiet `spacy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZr9Oy8P7SHE"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "R3ELyRc2zaJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfcttssX8Q16"
      },
      "source": [
        "Najpierw stokenizujmy tekst dla języka angielskiego:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4a1_8Tf8Vwc"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees. It's all we need to have some fun in life.\"\"\"\n",
        "\n",
        "tokens_en = nlp_en(text)\n",
        "[token.text for token in tokens_en]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A potem dla polskiego:\n",
        "(pamiętajmy o innej nazwie zmiennych!)"
      ],
      "metadata": {
        "id": "PRB7_tIjzujG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "4S6_NcsdzqvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y06DuFMVDmWO"
      },
      "source": [
        "A teraz podzielmy teksty na zdania."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq9wbNqGDoox",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "[sentence.text for sentence in tokens_en.sents]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "mFve78CiY9NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Zadanie sprawdzające umiejętności⭐\n",
        "\n",
        "Stwórz zdanie, które:\n",
        "- zawiera skróty (USA, U.S., i.e., ww.)\n",
        "- zawiera nazwy (McDonald's, Kelly's)\n",
        "- zawiera czasowniki w formach warunkowych, przypuszczających (I would like, chciałabym).\n",
        "\n",
        "```\n",
        "np. We have been to U.K. before we got to the very special country, i.e. Poland.\n",
        "```"
      ],
      "metadata": {
        "id": "mNlfmjT60NLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdKGB9UB-N3Z"
      },
      "outputs": [],
      "source": [
        "# Miejsce na Twój kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z39blSV0-sof"
      },
      "source": [
        "## Wykrywanie kategorii morfologicznych\n",
        "\n",
        "`spacy` można używać też do analizy kategorii morfologicznych (morfoskładniowych, zwanych też *częściami mowy*, które -- o zgrozo -- odmieniają się przez przypadki, osoby i stopnie). Tagowanie morfoskładniowe, inaczej Part-of-Speech Tagging (POS Tagging) przydaje się w bardziej zaawansowanych zadaniach, może też pozwolić na wnikliwy wgląd w właściwości tekstu.\n",
        "\n",
        "W tym zadaniu możemy użyć tokenów (`tokens_en`) z poprzednich ćwiczeń."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqCgDjRt-zMG"
      },
      "outputs": [],
      "source": [
        "[(token.text, token.pos_) for token in tokens_en]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "3_s6GPdoZJPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0J9krYr_qal"
      },
      "source": [
        "### ⭐ Zadanie sprawdzające umiejętności⭐\n",
        "\n",
        "**A teraz** zobaczmy, ile i jakich tagów (w tekstach można znaleźć też określenie POS tagów) mamy w naszych przykładach. Jakie problemy mogą być poruszone przy okazji takiego zadania? Jak zrobić wykres? (Za wykres jest bonus 📊 😀)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihx1WDro_wKH"
      },
      "outputs": [],
      "source": [
        "# Miejsce na Twój kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAgdEyZI_02h"
      },
      "source": [
        "Lematyzacja\n",
        "Wchodzimy na kolejny poziom abstrakcji. Liczymy teraz nie tokeny, a typy. Załóżmy, że chcemy policzyć dzieci z przedszkola bawiące się głośno na placu zabaw za naszym oknem. Chcemy dowiedzieć się, kto krzyczy głośniej, dziewczynki czy chłopcy. W tym celu przyglądamy się każdemu dziecku i stwierdzamy, czy jest chłopcem czy dziewczynką. Sprowadzamy patrzenie na obiekt do binarnego wyboru płci, czasem mamy wątpliwości, co jest normalne. Możemy (z wahaniem lub bez) powiedzieć, że na placu zabaw są dwa typy dzieci: krzyczące i nie, dziewczynki i chłopcy. I podobnie jest z lematyzacją, choć trochę inaczej. Żeby zobaczyć, co jest w zdaniu, musimy przyjrzeć się okazom i stwiedzić, że jeśli w tekście dziecko drze się (jakby jutra nie było), darło się (aż do momentu, kiedy nie zagłuszyły go syreny policyjne) lub będzie się darło (do ukończenia 18 roku życia), to mówimy o jednej czynności darcia się wyrażonej jako różne formy czasownika DRZEĆ SIĘ. To jak w słowniku. Jeśli chcemy sprawdzić pisownię, szukamy czegoś co jest w mianowniku liczby pojedynczej i rodzaju męskim, albo w bezokoliczniku, albo w stopniu równym i też rodzaju męskim.\n",
        "\n",
        "Co i kiedy liczymy? Wypisując wszystkie elementy (tokeny), liczyliśmy budulec tekstu. Wypisując lematy (typy), liczymy użycie konkretnych pojęć niezależnie od ich formy.\n",
        "\n",
        "Jeśli chcesz policzyć, ile słów zostało wymienionych w tekście, bardzo przydatne jest sprowadzenie wszystkich słów do ich form podstawowych. Proces ten nazywany jest lematyzacją. Tekst przetworzony za pomocą spacy zawiera już lematy dla każdego tokena. Wykorzystamy tę technikę w dalszej części laboratorium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIEpFin0_3cE"
      },
      "outputs": [],
      "source": [
        "[(token.text, token.lemma_) for token in tokens_en]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "AXk28TGz0r1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Y0w9TwHxiq"
      },
      "source": [
        "⭐ Zadanie sprawdzające umiejętności ⭐\n",
        "> Dla nietypowych rzeczowników w języku angielskim:\n",
        "\n",
        "* entities\n",
        "* was\n",
        "* mice\n",
        "*cacti\n",
        "* octopi\n",
        "\n",
        "znajdź lematy i oceń, czy spacy rozpoznał je poprawnie. Czy możesz wskazać analogiczne przykłady dla polskiego?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPMnY3xpH6ZT"
      },
      "outputs": [],
      "source": [
        "# Miejsce na Twój kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mih5AZl5_35c"
      },
      "source": [
        "## Named entity recognition\n",
        "\n",
        "Analiza tekstu przy użyciu `spacy` może być bardziej zaawansowana (do tej pory analizowaliśmy składnię, teraz czas na odrobinę semantyki). Takim bardziej złożonym zadaniem jest rozpoznawanie encji nazwanych (NER), a więc pewnego typu obiektów, które mogą być nazwą własną, ale wcale nie muszą.\n",
        "\n",
        "### Zatem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqAvaZj_Cgiv"
      },
      "outputs": [],
      "source": [
        "ner_result = nlp(\"\"\"Israel is being urged by the international community - including close ally the US - to do more to limit civilian casualties.\n",
        "\n",
        "Hamas officials say at least 16,248 people have been killed in Gaza since the start of the conflict, about three quarters of them women and children.\n",
        "\n",
        "Defending Israel's war strategy, former PM Naftali Bennett has told the BBC that Israel has been showing restraint in Gaza.\n",
        "\n",
        "If we wanted to harm civilians, we could have won the whole war in one day on October 8th, he said.\n",
        "\n",
        "We could have indiscriminately bombed Gaza.\n",
        "\n",
        "It could have been the easiest thing in the world... [but] we're not doing that.\"\"\")\n",
        "[(e.text, e.label_, e.start_char, e.end_char) for e in ner_result.ents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONbTGgU-Ex_0"
      },
      "source": [
        "Każda kategoria ma w SpaCy rozwinięcie:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGruXB_UEgHn"
      },
      "outputs": [],
      "source": [
        "spacy.explain('GPE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Woivn9naE9Al"
      },
      "source": [
        "⭐ Zadanie sprawdzające umiejętności ⭐\n",
        "\n",
        "> Zobacz na to samo zadanie, ale w języku polskim.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlkMZSw6FDPF"
      },
      "outputs": [],
      "source": [
        "# Miejsce na Twój kod"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⭐ Zadanie sprawdzające umiejętności ⭐\n",
        "\n",
        "Znajdź tekst, który zawiera typ `WORK_OF_ART`."
      ],
      "metadata": {
        "id": "9RO0Z_uJ1hNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "UgPECLjt1j3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RM3w_wFJsc"
      },
      "source": [
        "### Obrazki\n",
        "Moduł displacy odpowiada za wizualizację wyników działania NERa. Podkreślenie / wyróżnienie kolorystyczne sprawiają, że łatwiej odczytać wyniki, żeby pobieżnie je przeanalizować."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHBUbGOuFt2y"
      },
      "outputs": [],
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xNIdIyLGY0I"
      },
      "source": [
        "Korzystając z modułu `displacy` możemy też przeszukiwać informacje pod kątem jednej wybranej lub kilku pożądanych kateogrii. Żeby tego dokonać, musimy skorzystać z funkcji `displacy.render`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr8o6bPUGi5l"
      },
      "outputs": [],
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True, options={\"ents\": [\"PERSON\", \"DATE\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7UQMXj7GAaK"
      },
      "source": [
        "#### ⭐ Zadanie sprawdzające umiejętności ⭐\n",
        "\n",
        "Spróbuj przeanalizować dłuższy tekst za pomocą `spacy` i zwizualizuj wynik NER za pomocą `displacy`. Użyj jakiegoś artykułu znalezionego w sieci.\n",
        "\n",
        "Następnie policz ile razy każdy typ encji został wykryty w tekście i wyświetl statystyki. Dodatkowy bonus za wykres 📊 😀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaciXADYZ6Qp"
      },
      "outputs": [],
      "source": [
        "# Miejsce na Twój kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPYmmqLqG5YI"
      },
      "source": [
        "## Wykrywanie podobieństwa tekstów\n",
        "\n",
        "### Bag of words\n",
        "\n",
        "Do tej pory analizowaliśmy problem interpretacji formy tekstu, wskazywania konkretnych struktur i przypisywania ich do specyficznej klasy. Teraz zajmiemy się problemem interpretacji znaczenia. Przyjrzyjmy się różnym tekstom i sprawdźmy, jak te teksty są do siebie podobne. Dla uproszczenia tekstem będą pojedyncze zdania.\n",
        "\n",
        "> The quick brown fox jumps over the lazy dog.\n",
        "\n",
        "> The dog kept barking over the night.\n",
        "\n",
        "> A lazy fisherman with his dog met a fox last night.\n",
        "\n",
        "Przy niedużej liczbie tekstów jesteśmy w stanie naocznie porównać próbki i określić ich podobieństwo. Czym jest owo podobieństwo? Gdybyśmy mieli scharakteryzować sposoby, na jakie teksty są podobne, jakbyśmy je określili?\n",
        "\n",
        "Bardzo często stosowanym sposobem na znalezienie zdefiniowanego podobieństwa jest technika zwana *bag of words* (https://en.wikipedia.org/wiki/Bag-of-words_model). Polega ona na obliczeniu częstotliwości występowania słów we wszystkich tekstach, uporządkowaniu tekstu wg najpopularniejszych z nich, a następnie przedstawieniu tekstu jako listy liczb całkowitych zawierających liczbę wystąpień tych słów. Co ważne, to podejście zupełnie ignoruje\n",
        "\n",
        "Przykład lepszy niż wykład!\n",
        "\n",
        "Do obliczenia metryk tekstowych użyjemy modułu `sklearn`. Klasa `CountVectorizer` wykonuje wszystkie obliczenia za nas. Parametr `max_features=5` mówi wektoryzatorowi, że chcemy wybrać co najwyżej 5 najpopularniejszych tokenów ze wszystkich tekstów.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myGn2xGKI9hr"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog kept barking very loud barking and barking again over the night.\",\n",
        "    \"A lazy fisherman with his dog met a fox last night.\",\n",
        "]\n",
        "\n",
        "count_vector = CountVectorizer(max_features=5)\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "data_count.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "e6Y9j6R9cVNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMeNodp3Kj8y"
      },
      "source": [
        "OK, co oznaczają dane liczbowe, które uzyskaliśmy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0ABs1ziKyDA"
      },
      "outputs": [],
      "source": [
        "count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miejsce na Twój kod"
      ],
      "metadata": {
        "id": "1QLo1YXWc2cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fme7G6lDK1Eg"
      },
      "source": [
        "# Tokeny dla angielskiego to:\n",
        "\n",
        "```\n",
        "[ 'barking', 'dog', 'fox', 'over', 'the']\n",
        "```\n",
        "\n",
        "z reprezentacją wektorową:\n",
        "\n",
        "```\n",
        "array([[1, 1, 1, 0, 2],\n",
        "       [1, 0, 0, 1, 2],\n",
        "       [1, 1, 1, 1, 0]])\n",
        "```\n",
        "\n",
        "Co oznacza, że:\n",
        "* słowo `barking` pojawia się w ogóle trzy razy, ale w jednym zdaniu\n",
        "* słowo `dog` w pierwszym tekście pojawia się tylko raz, podobnie w trzecim\n",
        "* słowo `fox` pojawia się raz w pierwszym i raz w trzecim tekście\n",
        "* słowo `over` nie występuje w pierwszym tekście\n",
        "* słowo `the` pojawia się dwukrotnie w pierwszym i drugim tekście, w trzecim nie występuje wcale\n",
        "\n",
        "Now you should understand the *bag of words* text representation. We can say that the more similar the vectors are, the more similar the texts are, too. We can obviously calculate the distance between them and even visualize them on a chart, but we need a few more exercies and obviously - more data!\n",
        "\n",
        "#### ⭐ Zadania sprawdzające umiejętności ⭐\n",
        "\n",
        "Przeprowadź analogiczny eksperyment dla języka polskiego i wyciągnij wnioski podobnie jak w powyższym ćwiczeniu.\n",
        "\n",
        "Zwróć uwagę na parametr `max_features` i spróbuj zmieniać jego wartość, obserwując zmianę reprezentacji wyników. Wyciągnij wnioski na temat przełożenia wartości parametru na jakość prezentowanych wyników."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe2_07hOMKbO"
      },
      "outputs": [],
      "source": [
        "# Miejsce na Twój kod"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modele typu transformers korzystają z modyfikacji tego podejścia. Więcej o rozwiązaniu problemu podobieństwa tu: https://huggingface.co/tasks/sentence-similarity."
      ],
      "metadata": {
        "id": "UEB6xm4jPzaR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0EMySonML50"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Semantyczne rozważania nad językiem prowadzą do obserwacji, że słowa nie przekazują porównywalnie istotnych informacji. Co to znaczy? W przypadku przykładu z języka angielskiego, słowo `the` mówi nam nieco mniej niż słowo `dog` czy `lazy`, a jednak to ono pojawia się w tekstach najczęściej. Nie oznacza to oczywiście, że to słowo nic nie znaczy bądź nie pełni w systemie językowym istotnej funkcji. W tym zadaniu rozpatrujemy tylko istotność słowa w przełożeniu na znaczenie całego tekstu lub grupy tekstów i przez taki pryzmat będziemy patrzeć na `stopwords`, a więc słowa popularne, budulce tekstu, nienosące znaczenia.\n",
        "`Stopwords` obsługiwane są w pakiecie `SpaCy` przez moduł `sklearn` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), który identyfikuje je, a następnie usuwa z reprezentacji zbioru danych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkhFQzFdMs7U"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "list(ENGLISH_STOP_WORDS)[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHOezUD9OHnJ"
      },
      "source": [
        "Nie musisz importować stopwords, aby z nich korzystać, ponieważ są one zarządzane wewnętrznie w pakiecie (`_` w nazwie pakietu).\n",
        "\n",
        "Teraz wszystko, co musisz zrobić, to zdefiniować wbudowaną listę stopwords, których chcesz użyć przed obliczeniem wektorów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSzlqDvFOSbH"
      },
      "outputs": [],
      "source": [
        "count_vector = CountVectorizer(max_features=10, stop_words = 'english')\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L52dVF9qO_N3"
      },
      "source": [
        "### Wizualizacja danych\n",
        "\n",
        "\n",
        "Wykrywanie podobnych tekstów w przypadku dużej ilości danych może stanowić wyzwanie. Zawsze pomocna jest wizualizacja danych na ekranie, więc możemy wykreślić wektory i sprawdzić, czy możemy wykryć jakieś grupy na ekranie. Będzie to trudne w przypadku trzech tekstów, na których obecnie pracujemy, ale zrozumiesz ideę.\n",
        "\n",
        "Możemy teraz zawiesić to laboratorium i poczekać do 2048 roku, kiedy ekrany 5D będą dostępne, lub użyć popularnego algorytmu `t-SNE` do *spłaszczenia* danych, a następnie ich wizualizacji. Wybierzemy drugie rozwiązanie 😉.\n",
        "\n",
        "Nie zagłębiając się zbytnio w działanie tego algorytmu, jest on w stanie zredukować wektory XD do wektorów YD, z X>Y, zachowując odległości między nimi. W przypadku naszego tekstu chcemy zredukować wektory 5D (5 cech tekstu) do wektorów 2D (czyli do formatu, który można wykreślić na ekranie).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6c2W1S_QGIr"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne_model = TSNE(n_components=2, perplexity=2)\n",
        "tsne_data = tsne_model.fit_transform(data_count.toarray())\n",
        "\n",
        "tsne_data\n",
        "#data_count.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn5GI1iFQtr1"
      },
      "source": [
        "Algortym zredukował wektory, co możemy zobaczyć, zamiast musieć sobie to wyobrażać."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbwqxktaQ0gd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
        "\n",
        "for i, label in enumerate([\"quick fox\", \"barking dog\", \"lazy fisherman\"]):\n",
        "    ax.annotate(label, (tsne_data[i, 0], tsne_data[i, 1]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I60UgASRT70i"
      },
      "source": [
        "Istnieją tylko trzy punkty danych, więc trudno powiedzieć, czy teksty można uznać za podobne do siebie, czy nie. Gdybyśmy jednak mieli znacznie więcej tekstów, moglibyśmy podejrzewać, że punkty danych utworzyłyby pewne rozróżnialne grupy, co oznacza, że teksty mówią o podobnych tematach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvhm1-manrMN"
      },
      "source": [
        "## Zbiory danych\n",
        "\n",
        "Do ostatniego zadania potrzebujemy więcej danych, żeby zaobserwować proces klasteryzacji. Jedną z możliwych dróg pozyskania danych jest skorzystanie z modułu [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html) `datasets`, aby pobrać kilka tekstów, nad którymi możemy pracować.\n",
        "\n",
        "Zobaczmy, co jest w środku."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU9xQv2RoAs-"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "datasets.list_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wyhw7igoQdD"
      },
      "source": [
        "Zbiorów danych jest sporo, a ich liczba stale rośnie. Na potrzeby tego eksperymentu wybierzmy dowolny zbiór (zadanie nie jest tak wyspecyfikowane, by nakładać na nas konieczność wyboru wg konkretnych kryteriów).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GgJ05UnoclR"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset('ag_news', split='train')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU8czV0vqBkG"
      },
      "source": [
        "Jak widzieliśmy w poprzednich przykładach, lista tekstów będzie na razie łatwiejszą strukturą do pracy. Mając powyższy zbiór danych z polami `text` i `label`, możemy utworzyć listę tekstów z prostym zrozumieniem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GyDWshMqSHq"
      },
      "outputs": [],
      "source": [
        "large_texts = [item['text'] for item in dataset]\n",
        "large_texts[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYYikmVXUM9Y"
      },
      "source": [
        "## ⭐ Zadanie sprawdzające umiejętności 🗻 ⭐\n",
        "\n",
        "Masz wszystkie narzędzia!\n",
        "\n",
        "Zbierz duży zbiór danych tekstów z *HF* i:\n",
        "\n",
        "1.   Przygotuj je do analizy, np.\n",
        "  \n",
        "  a. Stokenizuj je.\n",
        "\n",
        "  b. Przekształć tokeny w lematy (tak, aby `dog` i `dogs` były traktowane jako ta sama funkcja).\n",
        "2. Przedstaw teksty jako bag of words, pamiętając o stopwords. Eksperymentuj z liczbą cech. Jeśli okaże się, że istnieją cechy, które wpływają na reprezentację, wróć do kroku 1. i weź to pod uwagę podczas przygotowywania danych.\n",
        "3. Zwizualizuj dane na wykresie (bez etykiet dla lepszej wydajności). Czy możesz wyróżnić jakieś grupy tekstów? O czym są te teksty?\n",
        "4. Wykryj nazwane jednostki w reprezentantach grup. Czy nazwane jednostki sugerują również temat tekstu?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkIYNxKC4D0c"
      },
      "outputs": [],
      "source": [
        "# Miejsce na Twój kod"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}