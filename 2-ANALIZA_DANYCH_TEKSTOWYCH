{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-ultimate-krol/winter_school/blob/main/2-ANALIZA_DANYCH_TEKSTOWYCH\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRvCuHz36wh9"
      },
      "source": [
        "# Analiza danych tekstowych -- wstƒôp\n",
        "\n",
        "## ≈örodowisko analityczne\n",
        "\n",
        "Przed Wami kr√≥tkie wprowadzenie do podstwowych zada≈Ñ z obszaru NLP. Wykonamy je na dostƒôpnych danych, ma≈Ço wymagajƒÖcym pipelinie i dla jƒôzyka angielskiego. Ale uwaga: analogiczne zadania bƒôdziecie robiƒá dla jƒôzyka polskiego (praca samodzielna, analogiczna do prezentowanej dla jƒôzyka angielskiego).\n",
        "\n",
        "Bƒôdziemy u≈ºywaƒá pakietu `spacy`, kt√≥ry sprawdza siƒô przy analizie tekstu, `scikit-learn` do oblicze≈Ñ i `matplotlip`, kt√≥ry pomo≈ºe zaprezentowaƒá dane na wykresach. Jako dane anglojƒôzyczne we≈∫miemy sobie `en_core_web_sm` --> model od SpaCy trenowany na tekstach news√≥w z jƒôzyka angielskiego. Pierwsze zadanie dla Was: Jaki pakiet danych odpowiada za dane z jƒôzyka polskiego?\n",
        "\n",
        "Modu≈Ç `datasets` odpowiada za ≈Çatwe ≈Çadowanie danych. Dane pochodzƒÖ z [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6vQYVbzRT2b"
      },
      "outputs": [],
      "source": [
        "!pip install spacy scikit-learn matplotlib datasets\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sPaOFZMRSku"
      },
      "source": [
        "## Tokenizacja\n",
        "\n",
        "Pierwsze zadanie polega na stokenizowaniu tekstu, co oznacza ni mniej ni wiƒôcej jak podzia≈Ç tekstu na najmniejsze znaczƒÖce elementy, kt√≥re wsp√≥lnie tworzƒÖ wyrazy tekstowe i tekst. Tokenizacja dla ka≈ºdego jƒôzyka jest inna, co mo≈ºecie zaobserwowaƒá zestawiajƒÖc dane dla jƒôzyka angielskiego z danymi dla jƒôzyka polskiego i por√≥wnujƒÖc wyniki uzyskane na tekstach.\n",
        "\n",
        "Tokenizacja przydaje siƒô w ≈ºyciu. PracujƒÖc na tokenach, pracujemy na uniwersalnych danych i por√≥wnujemy dane w spos√≥b niestronniczy.\n",
        "\n",
        "Por√≥wnajcie zdania:\n",
        "\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees. It's all we need to have some fun in life.\"\n",
        "\"Chcia≈Çabym, ≈ºeby≈õcie mieli tyle zabawy z ƒáwiczeniami, ile ubawu majƒÖ myszy z ka≈ºdym jednym kawa≈Çeczkiem sera. Wszystko, czego potrzebujemy w ≈ºyƒáku, to zabawa.\"\n",
        "\n",
        "\n",
        "Wyja≈õnienie:\n",
        "Dowiadujemy siƒô tu jak tokenizowaƒá tekst, a za chwilƒô dowiemy siƒô, ≈ºe te tokeny mogƒÖ byƒá przetwarzane w wektory, na podstawie kt√≥rych mo≈ºemy przewidywaƒá kolejne wyrazy tekstowe. Je≈õli jednak wynik tokenizacji nie ma byƒá u≈ºyty bezpo≈õrednio jako dane wej≈õciowe sieci neuronowej, mo≈ºemy u≈ºyƒá przyjaznej tokenizacji z pakietu `spacy`.\n",
        "\n",
        "Zainicjujmy wiƒôc pakiet `spacy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZr9Oy8P7SHE"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Miejsce na Tw√≥j kod"
      ],
      "metadata": {
        "id": "R3ELyRc2zaJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfcttssX8Q16"
      },
      "source": [
        "Najpierw stokenizujmy tekst dla jƒôzyka angielskiego:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4a1_8Tf8Vwc"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees. It's all we need to have some fun in life.\"\"\"\n",
        "\n",
        "tokens_en = nlp_en(text)\n",
        "[token.text for token in tokens_en]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A potem dla polskiego:\n",
        "(pamiƒôtajmy o innej nazwie zmiennych!)"
      ],
      "metadata": {
        "id": "PRB7_tIjzujG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Miejsce na Tw√≥j kod"
      ],
      "metadata": {
        "id": "4S6_NcsdzqvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y06DuFMVDmWO"
      },
      "source": [
        "A teraz podzielmy teksty na zdania."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq9wbNqGDoox",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "[sentence.text for sentence in tokens.sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci‚≠ê\n",
        "\n",
        "Stw√≥rz zdanie, kt√≥re:\n",
        "- zawiera skr√≥ty (USA, U.S., i.e., ww.)\n",
        "- zawiera nazwy (McDonald's, Kelly's)\n",
        "- zawiera czasowniki w formach warunkowych, przypuszczajƒÖcych (I would like, chcia≈Çabym).\n",
        "\n",
        "```\n",
        "np. We have been to U.K. before we got to the very special country, i.e. Poland.\n",
        "```"
      ],
      "metadata": {
        "id": "mNlfmjT60NLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdKGB9UB-N3Z"
      },
      "outputs": [],
      "source": [
        "### Miejsce na Tw√≥j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z39blSV0-sof"
      },
      "source": [
        "## Wykrywanie kategorii morfologicznych\n",
        "\n",
        "`spacy` mo≈ºna u≈ºywaƒá te≈º do analizy kategorii morfologicznych (morfosk≈Çadniowych, zwanych te≈º *czƒô≈õciami mowy*, kt√≥re -- o zgrozo -- odmieniajƒÖ siƒô przez przypadki, osoby i stopnie). Tagowanie morfosk≈Çadniowe, inaczej Part-of-Speech Tagging (POS Tagging) przydaje siƒô w bardziej zaawansowanych zadaniach, mo≈ºe te≈º pozwoliƒá na wnikliwy wglƒÖd w w≈Ça≈õciwo≈õci tekstu.\n",
        "\n",
        "W tym zadaniu mo≈ºemy u≈ºyƒá token√≥w (`tokens_en`) z poprzednich ƒáwicze≈Ñ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqCgDjRt-zMG"
      },
      "outputs": [],
      "source": [
        "[(token.text, token.pos_) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0J9krYr_qal"
      },
      "source": [
        "### ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci‚≠ê\n",
        "\n",
        "**A teraz** zobaczmy, ile i jakich tag√≥w (w tekstach mo≈ºna znale≈∫ƒá te≈º okre≈õlenie POS tag√≥w) mamy w naszych przyk≈Çadach. Jakie problemy mogƒÖ byƒá poruszone przy okazji takiego zadania? Jak zrobiƒá wykres? (Za wykres jest bonus üìä üòÄ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihx1WDro_wKH"
      },
      "outputs": [],
      "source": [
        "### Miejsce na Tw√≥j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAgdEyZI_02h"
      },
      "source": [
        "Lematyzacja\n",
        "Wchodzimy na kolejny poziom abstrakcji. Liczymy teraz nie tokeny, a typy. Za≈Ç√≥≈ºmy, ≈ºe chcemy policzyƒá dzieci z przedszkola bawiƒÖce siƒô g≈Ço≈õno na placu zabaw za naszym oknem. Chcemy dowiedzieƒá siƒô, kto krzyczy g≈Ço≈õniej, dziewczynki czy ch≈Çopcy. W tym celu przyglƒÖdamy siƒô ka≈ºdemu dziecku i stwierdzamy, czy jest ch≈Çopcem czy dziewczynkƒÖ. Sprowadzamy patrzenie na obiekt do binarnego wyboru p≈Çci, czasem mamy wƒÖtpliwo≈õci, co jest normalne. Mo≈ºemy (z wahaniem lub bez) powiedzieƒá, ≈ºe na placu zabaw sƒÖ dwa typy dzieci: krzyczƒÖce i nie, dziewczynki i ch≈Çopcy. I podobnie jest z lematyzacjƒÖ, choƒá trochƒô inaczej. ≈ªeby zobaczyƒá, co jest w zdaniu, musimy przyjrzeƒá siƒô okazom i stwiedziƒá, ≈ºe je≈õli w tek≈õcie dziecko drze siƒô (jakby jutra nie by≈Ço), dar≈Ço siƒô (a≈º do momentu, kiedy nie zag≈Çuszy≈Çy go syreny policyjne) lub bƒôdzie siƒô dar≈Ço (do uko≈Ñczenia 18 roku ≈ºycia), to m√≥wimy o jednej czynno≈õci darcia siƒô wyra≈ºonej jako r√≥≈ºne formy czasownika DRZEƒÜ SIƒò. To jak w s≈Çowniku. Je≈õli chcemy sprawdziƒá pisowniƒô, szukamy czego≈õ co jest w mianowniku liczby pojedynczej i rodzaju mƒôskim, albo w bezokoliczniku, albo w stopniu r√≥wnym i te≈º rodzaju mƒôskim.\n",
        "\n",
        "Co i kiedy liczymy? WypisujƒÖc wszystkie elementy (tokeny), liczyli≈õmy budulec tekstu. WypisujƒÖc lematy (typy), liczymy u≈ºycie konkretnych pojƒôƒá niezale≈ºnie od ich formy.\n",
        "\n",
        "Je≈õli chcesz policzyƒá, ile s≈Ç√≥w zosta≈Ço wymienionych w tek≈õcie, bardzo przydatne jest sprowadzenie wszystkich s≈Ç√≥w do ich form podstawowych. Proces ten nazywany jest lematyzacjƒÖ. Tekst przetworzony za pomocƒÖ spacy zawiera ju≈º lematy dla ka≈ºdego tokena. Wykorzystamy tƒô technikƒô w dalszej czƒô≈õci laboratorium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIEpFin0_3cE"
      },
      "outputs": [],
      "source": [
        "[(token.text, token.lemma_) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Miejsce na Tw√≥j kod"
      ],
      "metadata": {
        "id": "AXk28TGz0r1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Y0w9TwHxiq"
      },
      "source": [
        "‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
        "> Dla nietypowych rzeczownik√≥w w jƒôzyku angielskim:\n",
        "\n",
        "* entities\n",
        "* was\n",
        "* mice\n",
        "*cacti\n",
        "* octopi\n",
        "\n",
        "znajd≈∫ lematy i oce≈Ñ, czy spacy rozpozna≈Ç je poprawnie. Czy mo≈ºesz wskazaƒá analogiczne przyk≈Çady dla polskiego?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPMnY3xpH6ZT"
      },
      "outputs": [],
      "source": [
        "### Miejsce na Tw√≥j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mih5AZl5_35c"
      },
      "source": [
        "## Named entity recognition\n",
        "\n",
        "Analiza tekstu przy u≈ºyciu `spacy` mo≈ºe byƒá bardziej zaawansowana (do tej pory analizowali≈õmy sk≈Çadniƒô, teraz czas na odrobinƒô semantyki). Takim bardziej z≈Ço≈ºonym zadaniem jest rozpoznawanie encji nazwanych (NER), a wiƒôc pewnego typu obiekt√≥w, kt√≥re mogƒÖ byƒá nazwƒÖ w≈ÇasnƒÖ, ale wcale nie muszƒÖ.\n",
        "\n",
        "### Zatem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqAvaZj_Cgiv"
      },
      "outputs": [],
      "source": [
        "ner_result = nlp(\"\"\"Israel is being urged by the international community - including close ally the US - to do more to limit civilian casualties.\n",
        "\n",
        "Hamas officials say at least 16,248 people have been killed in Gaza since the start of the conflict, about three quarters of them women and children.\n",
        "\n",
        "Defending Israel's war strategy, former PM Naftali Bennett has told the BBC that Israel has been showing restraint in Gaza.\n",
        "\n",
        "If we wanted to harm civilians, we could have won the whole war in one day on October 8th, he said.\n",
        "\n",
        "We could have indiscriminately bombed Gaza.\n",
        "\n",
        "It could have been the easiest thing in the world... [but] we're not doing that.\"\"\")\n",
        "[(e.text, e.label_, e.start_char, e.end_char) for e in ner_result.ents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONbTGgU-Ex_0"
      },
      "source": [
        "Ka≈ºda kategoria ma w SpaCy rozwiniƒôcie:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGruXB_UEgHn"
      },
      "outputs": [],
      "source": [
        "spacy.explain('GPE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Woivn9naE9Al"
      },
      "source": [
        "‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
        "\n",
        "> Zobacz na to samo zadanie, ale w jƒôzyku polskim.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlkMZSw6FDPF"
      },
      "outputs": [],
      "source": [
        "### Miejsce na Tw√≥j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
        "\n",
        "Znajd≈∫ tekst, kt√≥ry zawiera typ `WORK_OF_ART`."
      ],
      "metadata": {
        "id": "9RO0Z_uJ1hNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Miejsce na Tw√≥j kod"
      ],
      "metadata": {
        "id": "UgPECLjt1j3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RM3w_wFJsc"
      },
      "source": [
        "### Obrazki\n",
        "Modu≈Ç displacy odpowiada za wizualizacjƒô wynik√≥w dzia≈Çania NERa. Podkre≈õlenie / wyr√≥≈ºnienie kolorystyczne sprawiajƒÖ, ≈ºe ≈Çatwiej odczytaƒá wyniki, ≈ºeby pobie≈ºnie je przeanalizowaƒá."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHBUbGOuFt2y"
      },
      "outputs": [],
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xNIdIyLGY0I"
      },
      "source": [
        "KorzystajƒÖc z modu≈Çu `displacy` mo≈ºemy te≈º przeszukiwaƒá informacje pod kƒÖtem jednej wybranej lub kilku po≈ºƒÖdanych kateogrii. ≈ªeby tego dokonaƒá, musimy skorzystaƒá z funkcji `displacy.render`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr8o6bPUGi5l"
      },
      "outputs": [],
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True, options={\"ents\": [\"PERSON\", \"DATE\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7UQMXj7GAaK"
      },
      "source": [
        "#### ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
        "\n",
        "Spr√≥buj przeanalizowaƒá d≈Çu≈ºszy tekst za pomocƒÖ `spacy` i zwizualizuj wynik NER za pomocƒÖ `displacy`. U≈ºyj jakiego≈õ artyku≈Çu znalezionego w sieci.\n",
        "\n",
        "Nastƒôpnie policz ile razy ka≈ºdy typ encji zosta≈Ç wykryty w tek≈õcie i wy≈õwietl statystyki. Dodatkowy bonus za wykres üìä üòÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaciXADYZ6Qp"
      },
      "outputs": [],
      "source": [
        "### Miejsce na Tw√≥j kod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPYmmqLqG5YI"
      },
      "source": [
        "## Detecting text similarity\n",
        "\n",
        "### Bag of words\n",
        "\n",
        "Let's say we have three texts.\n",
        "\n",
        "> The quick brown fox jumps over the lazy dog.\n",
        "\n",
        "> The dog kept barking over the night.\n",
        "\n",
        "> A lazy fisherman with his dog met a fox last night.\n",
        "\n",
        "How much they are similar to each other? Can we say they talking about similar topics?\n",
        "\n",
        "A very idiomatic way of finding this out is a technique called *bag of words*. Its based on the calculation of the frequency of words apearing in the all texts, selecting the most popular ones and then representing the text as a list of integers containing the number of appearances of these words.\n",
        "\n",
        "Example better than a lecture!\n",
        "\n",
        "We will use the `sklearn` module to calculate the text metrics. The `CountVectorizer` class does all of the calculations for us. The `max_features=5` parameter tells the vectorizer we want to select at most 5 the most popular tokens from all of the texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myGn2xGKI9hr"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog kept barking very loud barking and barking again over the night.\",\n",
        "    \"A lazy fisherman with his dog met a fox last night.\",\n",
        "]\n",
        "\n",
        "count_vector = CountVectorizer(max_features=5)\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "data_count.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMeNodp3Kj8y"
      },
      "source": [
        "Wooow! What does it even mean? Let's see the tokens that were chosen to describe the texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0ABs1ziKyDA"
      },
      "outputs": [],
      "source": [
        "count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fme7G6lDK1Eg"
      },
      "source": [
        "Ok, so the chosen tokens are\n",
        "\n",
        "```\n",
        "[ 'barking', 'dog', 'fox', 'over', 'the']\n",
        "```\n",
        "\n",
        "and the texts representation after creating the bag of words is:\n",
        "\n",
        "```\n",
        "array([[1, 1, 1, 0, 2],\n",
        "       [1, 0, 0, 1, 2],\n",
        "       [1, 1, 1, 1, 0]])\n",
        "```\n",
        "\n",
        "It means that:\n",
        "* the word `barking` appeared three times in general, but only in one sentence\n",
        "* the word `dog` appered in all of the texts once\n",
        "* the word `fox` appeared once in the first and the third text\n",
        "* the word `over` appeared once in the second and the third text\n",
        "* the word `the` appeared in the first and the second text, twice in both of them\n",
        "\n",
        "Now you should understand the *bag of words* text representation. We can say that the more similar the vectors are, the more similar the texts are, too. We can obviously calculate the distance between them and even visualize them on a chart, but we need a few more exercies and obviously - more data!\n",
        "\n",
        "#### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Try to experiment with the `max_features` option. What number of `max_features` results in best vectors according to you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe2_07hOMKbO"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0EMySonML50"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "As you saw, the word `the` also has been counted although it does not carry any information in the text. This can greatly influence the results of our analysis, so it's very common to remove such words from the text before calculating any metrics. These words are called *stopwords* and the `sklearn` module has built in mechanisms to remove them. Let's see some of them first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkhFQzFdMs7U"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "list(ENGLISH_STOP_WORDS)[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHOezUD9OHnJ"
      },
      "source": [
        "You don't need to import the stopwords to use them, because they are managed internally within the package (noticed the `_` in the package name?). However, you may find it interesting to see what's inside!\n",
        "\n",
        "Now, all you need to do is to define the builtin list of stopwords you want to use before calculating the vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSzlqDvFOSbH"
      },
      "outputs": [],
      "source": [
        "count_vector = CountVectorizer(max_features=10, stop_words='english')\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "count_vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L52dVF9qO_N3"
      },
      "source": [
        "### Visualization of the text vectors in the chart\n",
        "\n",
        "Detecting similar texts if you have a lot of data can be challenging. It's always helpful to visualize the data on the screen, so we could plot the vectors and see if we can detect some groups on the screen. It will be hard for three texts we are currently operating on, but you will get the idea.\n",
        "\n",
        "However, the screens are 2D only in 2023. We can now postpone this lab and wait to 2048 when he 5D screens will be available, or use the popular `t-SNE` algorithm to *flatten* the data and then visualize them. We will take the second solution!\n",
        "\n",
        "Without taking too deep into how this algorithm works, it is able to reduce the XD vectors to YD vectors, with X>Y, maintaining distances between them. For our text, we want to reduce 5D vectors (5 features of the text) to 2D vectors (so to the format that can be plotted on the screen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6c2W1S_QGIr"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne_model = TSNE(n_components=2, perplexity=2)\n",
        "tsne_data = tsne_model.fit_transform(data_count.toarray())\n",
        "\n",
        "tsne_data\n",
        "#data_count.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn5GI1iFQtr1"
      },
      "source": [
        "As you can see, the algorithm transformed all of the vectors into 2D. We can plot them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbwqxktaQ0gd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
        "\n",
        "for i, label in enumerate([\"quick fox\", \"barking dog\", \"lazy fisherman\"]):\n",
        "    ax.annotate(label, (tsne_data[i, 0], tsne_data[i, 1]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I60UgASRT70i"
      },
      "source": [
        "There are only three datapoints so it's hard to tell if the texts can be considered similar to each other or not. However, if we had many more texts, we might suspect that the data points would create some distinguishable groups, meaning the text are talking about similar topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvhm1-manrMN"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "We need more data for the final task. Luckily, there are many options for us to start with while learning. One option is to use the [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html) `datasets` module to download some texts we can work on.\n",
        "\n",
        "Let's see what's inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU9xQv2RoAs-"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "datasets.list_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wyhw7igoQdD"
      },
      "source": [
        "As you can see, there are many datasets we can work on. How to load them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GgJ05UnoclR"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset('ag_news', split='train')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU8czV0vqBkG"
      },
      "source": [
        "As we saw in the previous examples, the list of texts will be the easier structure to work on for now. Having the above dataset with `text` and `label` fields, we can create a list of texts with a simple comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GyDWshMqSHq"
      },
      "outputs": [],
      "source": [
        "large_texts = [item['text'] for item in dataset]\n",
        "large_texts[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYYikmVXUM9Y"
      },
      "source": [
        "## ‚≠ê A big üóª task for you ‚≠ê\n",
        "\n",
        "You have all the tools!\n",
        "\n",
        "Collect large dataset of texts from *HF* and:\n",
        "\n",
        "1.   Prepare them for analysis, e.g.\n",
        "  1. Tokenize them.\n",
        "  1. Transform the tokens into lemmas (so the `dog` and the `dogs` are treated as the same feature).\n",
        "2. Represent the texts as bag of words, remembering about stopwords. Experiment with the features count. If you find that there are features that influence the representation, go back to the step 1. and take it into consideration when preparing the data (maybe you want to get rid of numbers?).\n",
        "3. Visualize the data on a plot (without labels for better performance). Can you distiguish some groups of texts? What these texts are about?\n",
        "4. Detect named entites in groups representatives. Do named etities also suggest the topic of the text?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkIYNxKC4D0c"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}