{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2 - Text analysis\n",
        "\n",
        "## Environment\n",
        "\n",
        "We need `spacy` for text analysis, `scikit-learn` for calculations and `matplotlip` for charts and plots. We also need to download the `en_core_web_sm` spacy's language model we will work on.\n",
        "\n",
        "The `datasets` is a module to easily load datasets. They come from the [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html)."
      ],
      "metadata": {
        "id": "yRvCuHz36wh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy scikit-learn matplotlib datasets\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "b6vQYVbzRT2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "In the first lab you found out how to tokenize your text into tensors that can be further used to word predictions. If the tokenization result is not meant to be used as the neural network input directly, we can use much more friendly tokenizaion from the `spacy` package. For example, we may want to split text into word tokens.\n",
        "\n",
        "First, you need to import and initialize the `spacy` module."
      ],
      "metadata": {
        "id": "3sPaOFZMRSku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "vZr9Oy8P7SHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you can tokenize the text into sentences."
      ],
      "metadata": {
        "id": "AfcttssX8Q16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"The second lab will be exciting, too! \n",
        "There are many knowledge for you to gain like part of speech and\n",
        "named entities recognition or stemming. It will be fun!\"\"\"\n",
        "\n",
        "tokens = nlp(text)\n",
        "[token.text for token in tokens]\n"
      ],
      "metadata": {
        "id": "Y4a1_8Tf8Vwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or sentences:"
      ],
      "metadata": {
        "id": "y06DuFMVDmWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[sentence.text for sentence in tokens.sents]"
      ],
      "metadata": {
        "id": "Iq9wbNqGDoox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "It may seem that tokenization is just spliting the text by spaces or dots. But its smarter than that! Try to tokenize the following text into words.\n",
        "\n",
        "```\n",
        "We have been to U.K. before we got to the very special country, i.e. Poland.\n",
        "```"
      ],
      "metadata": {
        "id": "xUNMKUkU94S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "hdKGB9UB-N3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of speech detection\n",
        "\n",
        "You can use the `spacy` module to fetch information about part of speech (POS) of every token. We may use the `tokens` list initialized in the previous step."
      ],
      "metadata": {
        "id": "Z39blSV0-sof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(token.text, token.pos_) for token in tokens]"
      ],
      "metadata": {
        "id": "AqCgDjRt-zMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "**Now** go ahead and count how many different POS tags are there in the given text! We want to know how many verbs, adjectives, pronouns, etc. are there in the text. Extra bonus for a chart üìä üòÄ"
      ],
      "metadata": {
        "id": "h0J9krYr_qal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "Ihx1WDro_wKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "If you want to count how many certain word has been mentioned in the text, it is very useful to take all of the words to their base forms. This process is called as a *lemmatization*. The text processed with spacy already contains lemmas for every token. We will use this technique further in the lab."
      ],
      "metadata": {
        "id": "dAgdEyZI_02h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(token.text, token.lemma_) for token in tokens]"
      ],
      "metadata": {
        "id": "CIEpFin0_3cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Find lemmas for the following words:\n",
        "\n",
        "* entities\n",
        "* was\n",
        "* mice\n",
        "* cacti\n",
        "* octopi\n",
        "\n",
        "Are they lemmatized correctly with `spacy`?"
      ],
      "metadata": {
        "id": "d_Y0w9TwHxiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "oPMnY3xpH6ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named entity recognition\n",
        "\n",
        "Processing the text with `spacy` also results in recognizing named entities, i.e. **balblabla**.\n",
        "\n",
        "### Basics"
      ],
      "metadata": {
        "id": "Mih5AZl5_35c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_result = nlp(\"Questions are swirling around $30M nomination of Andrea Riseborough to Oscar at 30th January 2023 in U.S.\")\n",
        "[(e.text, e.label_, e.start_char, e.end_char) for e in ner_result.ents]"
      ],
      "metadata": {
        "id": "fqAvaZj_Cgiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wonder what the certain entity label means, you can ask `spacy` for an explanation."
      ],
      "metadata": {
        "id": "ONbTGgU-Ex_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "id": "wGruXB_UEgHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Try to come up with a text that will contain an entity of `WORK_OF_ART` type."
      ],
      "metadata": {
        "id": "Woivn9naE9Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "hlkMZSw6FDPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization\n",
        "\n",
        "You can use the `displacy` module of `spacy` to visualize the NER result. It will be much easier to analyze the text."
      ],
      "metadata": {
        "id": "Q0RM3w_wFJsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "XHBUbGOuFt2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also display only specific entity types for better text understanding. Read the docs for the `displacy.render` function to find out more options you can configure here."
      ],
      "metadata": {
        "id": "4xNIdIyLGY0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True, options={\"ents\": [\"MONEY\", \"DATE\"]})"
      ],
      "metadata": {
        "id": "lr8o6bPUGi5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Try to analyze some longer text with `spacy` and visualize the NER result with `displacy`. Use some artice found on the web.\n",
        "\n",
        "Then, count how many times each entity type has been detected in the text and display some stats. Extra bonus for a chart üìä üòÄ"
      ],
      "metadata": {
        "id": "r7UQMXj7GAaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "jaciXADYZ6Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting text similarity\n",
        "\n",
        "### Bag of words\n",
        "\n",
        "Let's say we have three texts.\n",
        "\n",
        "> The quick brown fox jumps over the lazy dog.\n",
        "\n",
        "> The dog kept barking over the night.\n",
        "\n",
        "> A lazy fisherman with his dog met a fox last night.\n",
        "\n",
        "How much they are similar to each other? Can we say they talking about similar topics? \n",
        "\n",
        "A very idiomatic way of finding this out is a technique called *bag of words*. Its based on the calculation of the frequency of words apearing in the all texts, selecting the most popular ones and then representing the text as a list of integers containing the number of appearances of these words.\n",
        "\n",
        "Example better than a lecture!\n",
        "\n",
        "We will use the `sklearn` module to calculate the text metrics. The `CountVectorizer` class does all of the calculations for us. The `max_features=5` parameter tells the vectorizer we want to select at most 5 the most popular tokens from all of the texts."
      ],
      "metadata": {
        "id": "iPYmmqLqG5YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog kept barking over the night.\",\n",
        "    \"A lazy fisherman with his dog met a fox last night.\",\n",
        "]\n",
        "\n",
        "count_vector = CountVectorizer(max_features=5)\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "data_count.toarray()"
      ],
      "metadata": {
        "id": "myGn2xGKI9hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wooow! What does it even mean? Let's see the tokens that were chosen to describe the texts."
      ],
      "metadata": {
        "id": "lMeNodp3Kj8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector.get_feature_names()"
      ],
      "metadata": {
        "id": "-0ABs1ziKyDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, so the chosen tokens are\n",
        "\n",
        "```\n",
        "['dog', 'fox', 'lazy', 'night', 'the']\n",
        "```\n",
        "\n",
        "and the texts representation after creating the bag of words is:\n",
        "\n",
        "```\n",
        "array([[1, 1, 1, 0, 2],\n",
        "       [1, 0, 0, 1, 2],\n",
        "       [1, 1, 1, 1, 0]])\n",
        "```\n",
        "\n",
        "It means that:\n",
        "* the word `dog` appered in all of the texts once\n",
        "* the word `fox` and `lazy` appeared once in the first and the third text\n",
        "* the word `night` appeared once in the second and the third text\n",
        "* the word `the` appeared in the first and the second text, twice in both of them\n",
        "\n",
        "Now you should understand the *bag of words* text representation. We can say that the more similar the vectors are, the more similar the texts are, too. We can obviously calculate the distance between them and even visualize them on a chart, but we need a few more exercies and obviously - more data!\n",
        "\n",
        "#### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Try to experiment with the `max_features` option. What number of `max_features` results in best vectors according to you?"
      ],
      "metadata": {
        "id": "fme7G6lDK1Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "qe2_07hOMKbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords\n",
        "\n",
        "As you saw, the word `the` also has been counted although it does not carry any information in the text. This can greatly influence the results of our analysis, so it's very common to remove such words from the text before calculating any metrics. These words are called *stopwords* and the `sklearn` module has built in mechanisms to remove them. Let's see some of them first."
      ],
      "metadata": {
        "id": "Q0EMySonML50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "list(ENGLISH_STOP_WORDS)[:10]"
      ],
      "metadata": {
        "id": "QkhFQzFdMs7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't need to import the stopwords to use them, because they are managed internally within the package (noticed the `_` in the package name?). However, you may find it interesting to see what's inside!\n",
        "\n",
        "Now, all you need to do is to define the builtin list of stopwords you want to use before calculating the vectors."
      ],
      "metadata": {
        "id": "eHOezUD9OHnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector = CountVectorizer(max_features=5, stop_words='english')\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "count_vector.get_feature_names()"
      ],
      "metadata": {
        "id": "rSzlqDvFOSbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1b3516-2a6a-48d6-ac3b-0a64fdcc0390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['barking', 'dog', 'fox', 'lazy', 'night']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of the text vectors in the chart\n",
        "\n",
        "Detecting similar texts if you have a lot of data can be challenging. It's always helpful to visualize the data on the screen, so we could plot the vectors and see if we can detect some groups on the screen. It will be hard for three texts we are currently operating on, but you will get the idea.\n",
        "\n",
        "However, the screens are 2D only in 2023. We can now postpone this lab and wait to 2048 when he 5D screens will be available, or use the popular `t-SNE` algorithm to *flatten* the data and then visualize them. We will take the second solution!\n",
        "\n",
        "Without taking too deep into how this algorithm works, it is able to reduce the XD vectors to YD vectors, with X>Y, maintaining distances between them. For our text, we want to reduce 5D vectors (5 features of the text) to 2D vectors (so to the format that can be plotted on the screen)."
      ],
      "metadata": {
        "id": "L52dVF9qO_N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne_model = TSNE(n_components=2)\n",
        "tsne_data = tsne_model.fit_transform(data_count.toarray())\n",
        "\n",
        "tsne_data"
      ],
      "metadata": {
        "id": "-6c2W1S_QGIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the algorithm transformed all of the vectors into 2D. We can plot them!"
      ],
      "metadata": {
        "id": "Zn5GI1iFQtr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
        "\n",
        "for i, label in enumerate([\"quick fox\", \"barking dog\", \"lazy fisherman\"]):\n",
        "    ax.annotate(label, (tsne_data[i, 0], tsne_data[i, 1]))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hbwqxktaQ0gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are only three datapoints so it's hard to tell if the texts can be considered similar to each other or not. However, if we had many more texts, we might suspect that the data points would create some distinguishable groups, meaning the text are talking about similar topics."
      ],
      "metadata": {
        "id": "I60UgASRT70i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets\n",
        "\n",
        "We need more data for the final task. Luckily, there are many options for us to start with while learning. One option is to use the [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html) `datasets` module to download some texts we can work on.\n",
        "\n",
        "Let's see what's inside."
      ],
      "metadata": {
        "id": "Bvhm1-manrMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "datasets.list_datasets()"
      ],
      "metadata": {
        "id": "CU9xQv2RoAs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, there are many datasets we can work on. How to load them?"
      ],
      "metadata": {
        "id": "9wyhw7igoQdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset('ag_news', split='train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "8GgJ05UnoclR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw in the previous examples, the list of texts will be the easier structure to work on for now. Having the above dataset with `text` and `label` fields, we can create a list of texts with a simple comprehension."
      ],
      "metadata": {
        "id": "oU8czV0vqBkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "large_texts = [item['text'] for item in dataset]\n",
        "large_texts[:10]"
      ],
      "metadata": {
        "id": "6GyDWshMqSHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚≠ê A big üóª task for you ‚≠ê\n",
        "\n",
        "You have all the tools!\n",
        "\n",
        "Collect large dataset of texts from *XXX* and:\n",
        "\n",
        "1.   Prepare them for analysis, e.g.\n",
        "  1. Tokenize them.\n",
        "  1. Transform the tokens into lemmas (so the `dog` and the `dogs` are treated as the same feature).\n",
        "2. Represent the texts as bag of words, remembering about stopwords. Experiment with the features count. If you find that there are features that influence the representation, go back to the step 1. and take it into consideration when preparing the data (maybe you want to get rid of numbers?).\n",
        "3. Visualize the data on a plot (without labels for better performance). Can you distiguish some groups of texts? What these texts are about?\n",
        "4. Detect named entites in groups representatives. Do named etities also suggest the topic of the text?\n"
      ],
      "metadata": {
        "id": "pYYikmVXUM9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "UINJCfLu0SSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "ppDTlhLZWGht"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}