{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2 - Text analysis\n",
        "\n",
        "## Environment\n",
        "\n",
        "We need `spacy` for text analysis, `scikit-learn` for calculations and `matplotlip` for charts and plots."
      ],
      "metadata": {
        "id": "yRvCuHz36wh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy scikit-learn matplotlib\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "b6vQYVbzRT2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d77092-3e27-4d1f-aff9-5e07f1265594"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-31 12:15:36.955799: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "In the first lab you found out how to tokenize your text into tensors that can be further used to word predictions. If the tokenization result is not meant to be used as the neural network input directly, we can use much more friendly tokenizaion from the `spacy` package. For example, we may want to split text into word tokens.\n",
        "\n",
        "First, you need to import and initialize the `spacy` module."
      ],
      "metadata": {
        "id": "3sPaOFZMRSku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "vZr9Oy8P7SHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you can tokenize the text into sentences."
      ],
      "metadata": {
        "id": "AfcttssX8Q16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"The second lab will be exciting, too! \n",
        "There are many knowledge for you to gain like part of speech and\n",
        "named entities recognition or stemming. It will be fun!\"\"\"\n",
        "\n",
        "tokens = nlp(text)\n",
        "[token.text for token in tokens]\n"
      ],
      "metadata": {
        "id": "Y4a1_8Tf8Vwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or sentences:"
      ],
      "metadata": {
        "id": "y06DuFMVDmWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[sentence.text for sentence in tokens.sents]"
      ],
      "metadata": {
        "id": "Iq9wbNqGDoox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "It may seem that tokenization is just spliting the text by spaces or dots. But its smarter than that! Try to tokenize the following text into words.\n",
        "\n",
        "```\n",
        "We have been to U.K. before we got to the very special country, i.e. Poland.\n",
        "```"
      ],
      "metadata": {
        "id": "xUNMKUkU94S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "hdKGB9UB-N3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of speech detection\n",
        "\n",
        "You can use the `spacy` module to fetch information about part of speech (POS) of every token. We may use the `tokens` list initialized in the previous step."
      ],
      "metadata": {
        "id": "Z39blSV0-sof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(token.text, token.pos_) for token in tokens]"
      ],
      "metadata": {
        "id": "AqCgDjRt-zMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "**Now** go ahead and count how many different POS tags are there in the given text! We want to know how many verbs, adjectives, pronouns, etc. are there in the text. Extra bonus for a chart üìä üòÄ"
      ],
      "metadata": {
        "id": "h0J9krYr_qal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "Ihx1WDro_wKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "If you want to count how many certain word has been mentioned in the text, it is very useful to take all of the words to their base forms. This process is called as a *lemmatization*. The text processed with spacy already contains lemmas for every token. We will use this technique further in the lab."
      ],
      "metadata": {
        "id": "dAgdEyZI_02h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(token.text, token.lemma_) for token in tokens]"
      ],
      "metadata": {
        "id": "CIEpFin0_3cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Find lemmas for the following words:\n",
        "\n",
        "* entities\n",
        "* was\n",
        "* ... \n",
        "\n",
        "Are they lemmatized correctly with `spacy`?"
      ],
      "metadata": {
        "id": "d_Y0w9TwHxiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "oPMnY3xpH6ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named entity recognition\n",
        "\n",
        "Processing the text with `spacy` also results in recognizing named entities, i.e. **balblabla**.\n",
        "\n",
        "### Basics"
      ],
      "metadata": {
        "id": "Mih5AZl5_35c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_result = nlp(\"Questions are swirling around $30M nomination of Andrea Riseborough to Oscar at 30th January 2023 in U.S.\")\n",
        "[(e.text, e.label_, e.start_char, e.end_char) for e in ner_result.ents]"
      ],
      "metadata": {
        "id": "fqAvaZj_Cgiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wonder what the certain entity label means, you can ask `spacy` for an explanation."
      ],
      "metadata": {
        "id": "ONbTGgU-Ex_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "id": "wGruXB_UEgHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Try to come up with a text that will contain an entity of `WORK_OF_ART` type."
      ],
      "metadata": {
        "id": "Woivn9naE9Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "hlkMZSw6FDPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization\n",
        "\n",
        "You can use the `displacy` module of `spacy` to visualize the NER result. It will be much easier to analyze the text."
      ],
      "metadata": {
        "id": "Q0RM3w_wFJsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "XHBUbGOuFt2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also display only specific entity types for better text understanding. Read the docs for the `displacy.render` function to find out more options you can configure here."
      ],
      "metadata": {
        "id": "4xNIdIyLGY0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True, options={\"ents\": [\"MONEY\", \"DATE\"]})"
      ],
      "metadata": {
        "id": "lr8o6bPUGi5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Try to analyze some longer text with `spacy` and visualize the NER result with `displacy`. Use some artice found on the web.\n",
        "\n",
        "Then, count how many times each entity type has been detected in the text and display some stats. Extra bonus for a chart üìä üòÄ"
      ],
      "metadata": {
        "id": "r7UQMXj7GAaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "jaciXADYZ6Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting text similarityg\n",
        "\n",
        "### Bag of words\n",
        "\n",
        "Let's say we have three texts.\n",
        "\n",
        "> The quick brown fox jumps over the lazy dog.\n",
        "\n",
        "> The dog kept barking over the night.\n",
        "\n",
        "> A lazy fisherman with his dog met a fox last night.\n",
        "\n",
        "How much they are similar to each other? Can we say they talking about similar topics? \n",
        "\n",
        "A very idiomatic way of finding this out is a technique called *bag of words*. Its based on the calculation of the frequency of words apearing in the all texts, selecting the most popular ones and then representing the text as a list of integers containing the number of appearances of these words.\n",
        "\n",
        "Example better than a lecture!\n",
        "\n",
        "We will use the `sklearn` module to calculate the text metrics. The `CountVectorizer` class does all of the calculations for us. The `max_features=5` parameter tells the vectorizer we want to select at most 5 the most popular tokens from all of the texts."
      ],
      "metadata": {
        "id": "iPYmmqLqG5YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog kept barking over the night.\",\n",
        "    \"A lazy fisherman with his dog met a fox last night.\",\n",
        "]\n",
        "\n",
        "count_vector = sklearn.feature_extraction.text.CountVectorizer(max_features=5)\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "data_count.toarray()"
      ],
      "metadata": {
        "id": "myGn2xGKI9hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wooow! What does it even mean? Let's see the tokens that were chosen to describe the texts."
      ],
      "metadata": {
        "id": "lMeNodp3Kj8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector.get_feature_names()"
      ],
      "metadata": {
        "id": "-0ABs1ziKyDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, so the chosen tokens are\n",
        "\n",
        "```\n",
        "['dog', 'fox', 'lazy', 'night', 'the']\n",
        "```\n",
        "\n",
        "and the texts representation after creating the bag of words is:\n",
        "\n",
        "```\n",
        "array([[1, 1, 1, 0, 2],\n",
        "       [1, 0, 0, 1, 2],\n",
        "       [1, 1, 1, 1, 0]])\n",
        "```\n",
        "\n",
        "It means that:\n",
        "* the word `dog` appered in all of the texts once\n",
        "* the word `fox` and `lazy` appeared once in the first and the third text\n",
        "* the word `night` appeared once in the second and the third text\n",
        "* the word `the` appeared in the first and the second text, twice in both of them\n",
        "\n",
        "Now you should understand the *bag of words* text representation. We can say that the more similar the vectors are, the more similar the texts are, too. We can obviously calculate the distance between them and even visualize them on a chart, but we need a few more exercies and obviously - more data!\n",
        "\n",
        "#### ‚≠ê Task for you ‚≠ê\n",
        "\n",
        "Try to experiment with the `max_features` option. What number of `max_features` results in best vectors according to you?"
      ],
      "metadata": {
        "id": "fme7G6lDK1Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "qe2_07hOMKbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords\n",
        "\n",
        "As you saw, the word `the` also has been counted although it does not carry any information in the text. This can greatly influence the results of our analysis, so it's very common to remove such words from the text before calculating any metrics. These words are called *stopwords* and the `sklearn` module has built in mechanisms to remove them. Let's see some of them first."
      ],
      "metadata": {
        "id": "Q0EMySonML50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "list(ENGLISH_STOP_WORDS)[:10]"
      ],
      "metadata": {
        "id": "QkhFQzFdMs7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't need to import the stopwords to use them, because they are managed internally within the package (notice the `_` in the package name). However, you may find it interesting to see what's inside!\n",
        "\n",
        "Now, all you need to do is to define the builtin list of stopwords you want to use before calculating the vectors."
      ],
      "metadata": {
        "id": "eHOezUD9OHnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector = sklearn.feature_extraction.text.CountVectorizer(max_features=5, stop_words='english')\n",
        "data_count = count_vector.fit_transform(texts)\n",
        "count_vector.get_feature_names()"
      ],
      "metadata": {
        "id": "rSzlqDvFOSbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of the text vectors in the chart\n",
        "\n",
        "Detecting similar texts if you have a lot of data can be challenging. It's always helpful to visualize the data on the screen, so we could plot the vectors and see if we can detect some groups on the screen. It will be hard for three texts we are currently operating on, but you will get the idea.\n",
        "\n",
        "However, the screen is 2D only in 2023. We can now postpone this lab and wait to 2048 when he 5D screens will be available, or use the popular `t-SNE` algorithm to *flatten* the data to visualize them. We will take the second solution!\n",
        "\n",
        "Without taking too deep into how this algorithm works, it is able to reduce the XD vectors to YD vectors, with X>Y, maintaining distances between them. For our text, we want to reduce 5D vectors (5 features of the text) to 2D vectors (so, data that can be plotted on the screen)."
      ],
      "metadata": {
        "id": "L52dVF9qO_N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne_model = TSNE(n_components=2)\n",
        "tsne_data = tsne_model.fit_transform(data_count.toarray())\n",
        "\n",
        "tsne_data"
      ],
      "metadata": {
        "id": "-6c2W1S_QGIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the algorithm transformed all of the vectors into 2D. We can plot them!"
      ],
      "metadata": {
        "id": "Zn5GI1iFQtr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
        "\n",
        "for i, label in enumerate([\"quick fox\", \"barking dog\", \"lazy fisherman\"]):\n",
        "    ax.annotate(label, (tsne_data[i, 0], tsne_data[i, 1]))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hbwqxktaQ0gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are only three datapoints so it's hard to tell if the text can be considered similar to each other or not. However, if we had many more texts, we might suspect that the data points would create some distinguishable groups meaning the text are talking about similar topics."
      ],
      "metadata": {
        "id": "I60UgASRT70i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚≠ê A big üóª task for you ‚≠ê\n",
        "\n",
        "You have all the tools!\n",
        "\n",
        "Collect large dataset of texts from *XXX* and:\n",
        "\n",
        "1.   Prepare them for analysis, e.g.\n",
        "  1. Tokenize them.\n",
        "  1. Transform the tokens into lemmas.\n",
        "2. Represent the texts as bag of words, remembering about stopwords. Experiment with the features count. If you find that there are features that influence the representation, go back to the step 1. and take it into consideration when preparing the data (maybe you want to get rid of numbers?).\n",
        "3. Visualize the data on a plot (without labels for better performance). Can you distiguish some groups of texts? What these texts are about?\n",
        "4. Detected named entites in groups representatives. Do named etities also suggest the topic of the text?\n"
      ],
      "metadata": {
        "id": "pYYikmVXUM9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "ppDTlhLZWGht"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}