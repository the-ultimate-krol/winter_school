{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bad4bfa-ebdd-4102-a75e-4c714e51d4ce",
   "metadata": {},
   "source": [
    "# Natural language processing\n",
    "\n",
    "## Introduction to Transformers Architecture\n",
    "Currently, the most popular models for Natural Language Processing use the Transformer Architecture. There are several libraries implementing this architecture. However, in the context of NLP Huggingface transformers are most commonly used.\n",
    "\n",
    "Apart from the source code itself, this library contains a number of other elements. Among the most important of these are:\n",
    "\n",
    "[models](https://huggingface.co/models) - a huge and growing number of ready-made models that we can use to solve many problems in NLP (but also in speech recognition or image processing),\n",
    "[datasets](https://huggingface.co/datasets) - a very large catalogue of useful datasets that we can easily use to train our own NLP models (and other models).\n",
    "\n",
    "## Environment preparation - How to start with Google Colab\n",
    "\n",
    "Training NLP models requires access to hardware accelerators to accelerate the learning of neural networks. If our computer is not equipped with a GPU, we can use the Google Colab environment.\n",
    "\n",
    "In this environment, we can choose an accelerator from GPU and TPU. Let us check if we have access to an environment equipped with an NVidia accelerator by executing the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e3d78cb-b5b5-40cc-bf3c-b5dc08de9345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e7ecc-0521-4093-9cbd-84c20ac3b851",
   "metadata": {},
   "source": [
    "If the accelerator is unavailable (the command ended with an error), we change the execution environment by selecting from the \"Execution environment\" menu -> \"Change execution environment type\" -> GPU.\n",
    "\n",
    "We will then install all the necessary libraries. In addition to the `transformers` library itself, we also install the `datasets` management library datasets, a library that defines many metrics used in AI `evaluate` algorithms, and additional tools such as `sacremoses` and `sentencepiece`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c5e836-fb0f-4d6a-8f9d-efc5d5c83403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sacremoses datasets evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ecc9d-75dc-42d4-b54e-6da8dcfb0ba8",
   "metadata": {},
   "source": [
    "With the necessary libraries installed, we can use all the models and datasets registered in the catalogue.\n",
    "\n",
    "A typical way of using the available models is:\n",
    "\n",
    "- using a ready-made model that performs a specific task, e.g. [sentiment analysis](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis) - a model of this kind does not need to be trained, it is enough to run it in order to obtain a classification result (this can be seen in the demo at the indicated link),\n",
    "- using a base model that is trained for a specific task; an example of such a model is the [HerBERT base](https://huggingface.co/allegro/herbert-base-cased), which was taught as a masked language model. To use it for a specific task, we need to select a 'classification head' for it and retrain it on our own dataset.\n",
    "\n",
    "Models of this kind are different, and can be loaded using a common interface, but it is best to use one of the specialised classes, tailored to the task at hand. We will start by loading the BERT base model - one of the most popular models, for English. We will use it to guess missing words in the text. We will use the `AutoModelForMaskedLM` call to do this.\n",
    "Use the code to see the outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee1cd5c-3c4f-4f4f-80a4-2c0a90de9419",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForMaskedLM, AutoTokenizer\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b372b422-42a9-4ef8-9c00-013fc343c8d0",
   "metadata": {},
   "source": [
    "## Connecting Google Drive\n",
    "The final element of preparation, which is optional, is to attach your own Google Drive to the Colab environment. This makes it possible to save trained models, during the training process, to an \"external\" drive. If Google Colab leads to an interruption of the training process, the files that were successfully saved during the training will nevertheless not be lost. It will be possible to resume training already on a partially trained model.\n",
    "\n",
    "\n",
    "To do this, we mount the Google Drive in Colab. This requires authorisation of the Colab tool in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a8a39-abb1-4994-a23d-15e911bf6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01902f89-088c-4de9-9883-6aef8c65d769",
   "metadata": {},
   "source": [
    "Once the drive is mounted, we have access to the entire contents of Google Drive. When indicating where to save data during a workout, indicate a path starting with `/content/gdrive`, but indicate some subdirectory within our drive space. The full path could be `/content/gdrive/MyDrive/output`. It is a good idea to check that the data writes to the drive before running the workout.\n",
    "\n",
    "## Text tokenization\n",
    "Loading the model itself, however, is not enough to start using it. We must have a mechanism for converting text (a string of characters), into a sequence of tokens, belonging to a specific dictionary. During the training of the model, this dictionary is determined (selected algorithmically) before the actual training of the neural network. Although it is possible to extend it later (training on the training data, it also allows to obtain a representation of missing tokens), usually the dictionary in the form that was defined before the neural network training is used. Therefore, it is important to specify the correct dictionary for the tokeniser performing the text splitting.\n",
    "\n",
    "The library has an `AutoTokenizer` class that accepts the model name, which allows the dictionary corresponding to the selected neural network model to be automatically loaded. However, it is important to remember that if you are using 2 models, each will most likely have a different dictionary, and therefore they must have their own instances of the `Tokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19523e67-58f5-42ae-8d57-52fe82469d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97.tar.gz (524 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.24.1-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp311-cp311-win_amd64.whl (267 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: six in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (1.16.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Using cached pyarrow-10.0.1-cp311-cp311-win_amd64.whl (20.2 MB)\n",
      "Collecting dill<0.3.7\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.5.2-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.3-cp311-cp311-win_amd64.whl (317 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.2-cp311-cp311-win_amd64.whl (55 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp311-cp311-win_amd64.whl (32 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Installing collected packages: sentencepiece, xxhash, typing-extensions, tqdm, regex, numpy, multidict, joblib, fsspec, frozenlist, filelock, dill, click, charset-normalizer, async-timeout, yarl, sacremoses, pyarrow, pandas, multiprocess, aiosignal, responses, huggingface-hub, aiohttp, transformers, datasets, evaluate\n",
      "  Running setup.py install for sentencepiece: started\n",
      "  Running setup.py install for sentencepiece: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: sentencepiece is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for sentencepiece did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\AGH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  copying src\\sentencepiece/__init__.py -> build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  copying src\\sentencepiece/_version.py -> build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  copying src\\sentencepiece/sentencepiece_model_pb2.py -> build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  copying src\\sentencepiece/sentencepiece_pb2.py -> build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  running build_ext\n",
      "  building 'sentencepiece._sentencepiece' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "sentencepiece\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6c47c-d333-45bf-b95a-2e3743ed9793",
   "metadata": {},
   "source": [
    "The Tokenizer uses a fixed-size dictionary. This, of course, subordinates to the fact that not all words occurring in the text will be included. Furthermore, if we use the tokenizer to split text in a language other than the one for which it was created, such text will be split into a larger number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc1dbc-b6bd-4ac4-bbe5-63decb249538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
