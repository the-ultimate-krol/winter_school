{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bad4bfa-ebdd-4102-a75e-4c714e51d4ce",
   "metadata": {},
   "source": [
    "# Natural language processing\n",
    "\n",
    "## Introduction to Transformers Architecture\n",
    "Currently, the most popular models for Natural Language Processing use the Transformer Architecture. There are several libraries implementing this architecture. However, in the context of NLP Huggingface transformers are most commonly used.\n",
    "\n",
    "Apart from the source code itself, this library contains a number of other elements. Among the most important of these are:\n",
    "\n",
    "[models](https://huggingface.co/models) - a huge and growing number of ready-made models that we can use to solve many problems in NLP (but also in speech recognition or image processing),\n",
    "[datasets](https://huggingface.co/datasets) - a very large catalogue of useful datasets that we can easily use to train our own NLP models (and other models).\n",
    "\n",
    "## Environment preparation - How to start with Google Colab\n",
    "\n",
    "Training NLP models requires access to hardware accelerators to accelerate the learning of neural networks. If our computer is not equipped with a GPU, we can use the Google Colab environment.\n",
    "\n",
    "In this environment, we can choose an accelerator from GPU and TPU. Let us check if we have access to an environment equipped with an NVidia accelerator by executing the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d78cb-b5b5-40cc-bf3c-b5dc08de9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e7ecc-0521-4093-9cbd-84c20ac3b851",
   "metadata": {},
   "source": [
    "If the accelerator is unavailable (the command ended with an error), we change the execution environment by selecting from the \"Execution environment\" menu -> \"Change execution environment type\" -> GPU.\n",
    "\n",
    "We will then install all the necessary libraries. In addition to the `transformers` library itself, we also install the `datasets` management library datasets, a library that defines many metrics used in AI `evaluate` algorithms, and additional tools such as `sacremoses` and `sentencepiece`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5e836-fb0f-4d6a-8f9d-efc5d5c83403",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sacremoses datasets evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ecc9d-75dc-42d4-b54e-6da8dcfb0ba8",
   "metadata": {},
   "source": [
    "With the necessary libraries installed, we can use all the models and datasets registered in the catalogue.\n",
    "\n",
    "A typical way of using the available models is:\n",
    "\n",
    "- using a ready-made model that performs a specific task, e.g. [sentiment analysis](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis) - a model of this kind does not need to be trained, it is enough to run it in order to obtain a classification result (this can be seen in the demo at the indicated link),\n",
    "- using a base model that is trained for a specific task; an example of such a model is the [HerBERT base](https://huggingface.co/allegro/herbert-base-cased), which was taught as a masked language model. To use it for a specific task, we need to select a 'classification head' for it and retrain it on our own dataset.\n",
    "\n",
    "Models of this kind are different, and can be loaded using a common interface, but it is best to use one of the specialised classes, tailored to the task at hand. We will start by loading the BERT base model - one of the most popular models, for English. We will use it to guess missing words in the text. We will use the `AutoModelForMaskedLM` call to do this.\n",
    "Use the code to see the outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1cd5c-3c4f-4f4f-80a4-2c0a90de9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b372b422-42a9-4ef8-9c00-013fc343c8d0",
   "metadata": {},
   "source": [
    "## Connecting Google Drive\n",
    "The final element of preparation, which is optional, is to attach your own Google Drive to the Colab environment. This makes it possible to save trained models, during the training process, to an \"external\" drive. If Google Colab leads to an interruption of the training process, the files that were successfully saved during the training will nevertheless not be lost. It will be possible to resume training already on a partially trained model.\n",
    "\n",
    "\n",
    "To do this, we mount the Google Drive in Colab. This requires authorisation of the Colab tool in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a8a39-abb1-4994-a23d-15e911bf6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01902f89-088c-4de9-9883-6aef8c65d769",
   "metadata": {},
   "source": [
    "Once the drive is mounted, we have access to the entire contents of Google Drive. When indicating where to save data during a workout, indicate a path starting with `/content/gdrive`, but indicate some subdirectory within our drive space. The full path could be `/content/gdrive/MyDrive/output`. It is a good idea to check that the data writes to the drive before running the workout.\n",
    "\n",
    "## Text tokenization\n",
    "Loading the model itself, however, is not enough to start using it. We must have a mechanism for converting text (a string of characters), into a sequence of tokens, belonging to a specific dictionary. During the training of the model, this dictionary is determined (selected algorithmically) before the actual training of the neural network. Although it is possible to extend it later (training on the training data, it also allows to obtain a representation of missing tokens), usually the dictionary in the form that was defined before the neural network training is used. Therefore, it is important to specify the correct dictionary for the tokeniser performing the text splitting.\n",
    "\n",
    "The library has an `AutoTokenizer` class that accepts the model name, which allows the dictionary corresponding to the selected neural network model to be automatically loaded. However, it is important to remember that if you are using 2 models, each will most likely have a different dictionary, and therefore they must have their own instances of the `Tokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19523e67-58f5-42ae-8d57-52fe82469d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6c47c-d333-45bf-b95a-2e3743ed9793",
   "metadata": {},
   "source": [
    "The Tokenizer uses a fixed-size dictionary. This, of course, subordinates to the fact that not all words occurring in the text will be included. Furthermore, if we use the tokenizer to split text in a language other than the one for which it was created, such text will be split into a larger number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc1dbc-b6bd-4ac4-bbe5-63decb249538",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = tokenizer.encode(\n",
    "    \"The quick brown fox jumps over the lazy dog.\", return_tensors=\"pt\"\n",
    ")\n",
    "print(sentence1)\n",
    "print(sentence1.shape)\n",
    "\n",
    "sentence2 = tokenizer.encode(\"Zażółć gęślą jaźń.\", return_tensors=\"pt\")\n",
    "print(sentence2)\n",
    "print(sentence2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ecb1a-b415-4c43-9a78-e22c0688960a",
   "metadata": {},
   "source": [
    "(Using the tokenizer for English to split any other language sentence, we see that we get a much larger number of tokens. To see how the tokenizer has split the text, we can use the call `covert_ids_to_tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b1473-4902-4995-9883-1ec4c6efb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence1[0]))))\n",
    "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence2[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169458a-9707-4d17-a6b3-6bf00e452766",
   "metadata": {},
   "source": [
    "We can see that for English, all the words in the sentence have been converted into single tokens. In the case of a sentence in any  other language containing a number of diacritical signs, the situation is completely different - each sign has been extracted into a separate sub-token. The fact that we are dealing with sub-tokens is signalled by two crosses preceding a given sub-token. These indicate that this sub-token must be glued together with the preceding token to obtain the correct character string.)\n",
    "\n",
    "## Excercise 1\n",
    "\n",
    "Use the tokenizer for `xlm-roberta-large` to tokenize the same sentences. What conclusions can be drawn by looking at how tokenisation is done using different dictionaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0231da-b2d5-4840-af4e-12d26c1036ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49980786-5525-442b-9c9d-26b4e0a2cda4",
   "metadata": {},
   "source": [
    "As an outcome of the tokenization, beside words/tokens present in the original text, additional [CLS] and [SEP] tags (or other tags - depending on the dictionary used) appear in the tokenisation results. These have a special meaning and can be used to perform specific functions related to text analysis. For example, the [CLS] token representation is used in sentence classification tasks. The token [SEP], on the other hand, is used to distinguish between sentences, in tasks requiring two sentences as an input (e.g. determining how similar the sentences are to each other).\n",
    "\n",
    "##Language modelling\n",
    "\n",
    "Models pretreated in the self-supervised learning (SSL) regime do not have special capabilities for solving specific natural language processing tasks, such as answering questions or classifying text (except for very large models such as GPT-3, for example). However, they can be used to determine the probability of words in a text, and thus to test how much knowledge a specific model has in terms of language knowledge, or general knowledge of the world.\n",
    "\n",
    "In order to check how the model performs in these tasks, we can perform inference on the input data, in which some words will be replaced by special masking symbols used during the pre-training of the model.\n",
    "\n",
    "Keep in mind that different models may use different special sequences during pretraining. For example, Bert uses the sequence [MASK]. We can check the appearance of the mask token or its identifier in [the tokeniser configuration file](https://huggingface.co/bert-base-cased/raw/main/tokenizer.json) distributed with the model.\n",
    "\n",
    "As a first step, we will try to fill in the missing word in the English sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a217a7-a532-48c0-8cea-c8062019a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_en = tokenizer.encode(\n",
    "    \"The quick brown [MASK] jumps over the lazy dog.\", return_tensors=\"pt\"\n",
    ")\n",
    "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence_en[0]))))\n",
    "target = model(sentence_en)\n",
    "print(target.logits[0][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9fd9a-cb84-46ed-a8f7-1768e9ceaea5",
   "metadata": {},
   "source": [
    "Since the sentence is completed with the `[CLS]` tag after stocenisation, the masked word is in position 4. The `call target.logits[0][4]` shows a tensor with the probability distribution of the individual words, which was determined from the model parameters. We can select the words that have the highest probability using the call `torch.topk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85c49a-38f5-4c47-a346-712d758ceb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "top = torch.topk(target.logits[0][4], 5)\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4ce04-e2b7-4370-a2c6-56836f39e037",
   "metadata": {},
   "source": [
    "We obtained two vectors - `values` containing the components of the output vector of the neural network (unnormalised) and `indices`containing the indices of these components. From this, we can display the expression that the model believes are the most likely complements of the masked expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03276c-d635-4960-b29a-df539d63fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.convert_ids_to_tokens(top.indices)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(words, top.values.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92b9e1-0b75-41ef-8b76-8c7b7857adfb",
   "metadata": {},
   "source": [
    "As expected, the most likely replacement for the missing word is dog. The second word ##ie may be a little surprising, but when added to the existing text we get the sentence 'The quick brownie jumps over the lazy dog', which also seems sensible (if a little surprising).\n",
    "\n",
    "## Excercise 2\n",
    "\n",
    "Using the `xlm-roberta-model`, propose sentences with one missing word, verifying the ability of this model to:\n",
    "\n",
    "accommodate meaning in semantic context,\n",
    "account for long-distance relationships in a text,\n",
    "represent knowledge about the world.\n",
    "For each problem, come up with 3 test sentences and display the prediction for the 5 most likely words.\n",
    "\n",
    "Please try to come up with examples having masked item in different positions within the sentence.\n",
    "\n",
    "You can use the code from the `plot_words` function to help you display the results. Also, verify what masking token is used in this model, and remember to load the `xlm-roberta-model`.\n",
    "\n",
    "Evaluate the model's capabilities for the tasks indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917224d9-18e9-4541-a469-e5f223daf05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(sentence, word_model, word_tokenizer, mask=\"[MASK]\"):\n",
    "    sentence = word_tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    tokens = word_tokenizer.convert_ids_to_tokens(list(sentence[0]))\n",
    "    print(\"|\".join(tokens))\n",
    "    target = word_model(sentence)\n",
    "    top = torch.topk(target.logits[0][tokens.index(mask)], 5)\n",
    "    words = word_tokenizer.convert_ids_to_tokens(top.indices)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.bar(words, top.values.detach().numpy())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
